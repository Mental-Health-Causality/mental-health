{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-11 15:02:46,495 - /homeLocal/miniconda3/envs/cml/lib/python3.8/site-packages/castle/backend/__init__.py[line:36] - INFO: You can use `os.environ['CASTLE_BACKEND'] = backend` to set the backend(`pytorch` or `mindspore`).\n",
      "2022-12-11 15:02:46,585 - /homeLocal/miniconda3/envs/cml/lib/python3.8/site-packages/castle/algorithms/__init__.py[line:36] - INFO: You are using ``pytorch`` as the backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import dataset utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import importlib\n",
    "if importlib.util.find_spec('ipywidgets') is not None:\n",
    "    from tqdm.auto import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "from castle.common import GraphDAG\n",
    "from castle.metrics import MetricsDAG\n",
    "from castle.datasets import IIDSimulation, DAG\n",
    "from castle.algorithms import PC, Notears\n",
    "\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-11 15:02:54,863 - /homeLocal/miniconda3/envs/cml/lib/python3.8/site-packages/castle/datasets/simulator.py[line:270] - INFO: Finished synthetic dataset\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data simulation, simulate true causal dag and train_data.\n",
    "weighted_random_dag = DAG.erdos_renyi(n_nodes=10, n_edges=11,\n",
    "                                      weight_range=(0.5, 2.0), seed=1)\n",
    "dataset = IIDSimulation(W=weighted_random_dag, n=20000, method='linear', \n",
    "                        sem_type='gauss')\n",
    "true_causal_matrix, node_data = dataset.B, dataset.X\n",
    "\n",
    "G = nx.DiGraph(true_causal_matrix)\n",
    "nx.draw(G, with_labels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset from X \n",
    "class CausalDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP with variable number of layers using Sequential\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.layers = nn.Sequential()\n",
    "        self.layers.add_module('input', nn.Linear(self.input_dim, self.hidden_dim))\n",
    "        self.layers.add_module('relu1', nn.ReLU())\n",
    "        for i in range(self.n_layers - 1):\n",
    "            self.layers.add_module('hidden' + str(i), nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "            self.layers.add_module('relu' + str(i + 2), nn.ReLU())\n",
    "        self.layers.add_module('output', nn.Linear(self.hidden_dim, self.output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "# Train model\n",
    "def train(model, train_loader, criterion, optimizer, epochs, device, verbose=True):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    # pbar = tqdm(range(epochs))\n",
    "    for epoch in range(epochs):\n",
    "        mean_epoch_loss = []\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            mean_epoch_loss.append(loss.item())\n",
    "        mean_epoch_loss = np.mean(mean_epoch_loss)\n",
    "        # pbar.set_description(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}')\n",
    "        # pbar.update(1)\n",
    "        train_loss.append(mean_epoch_loss)\n",
    "\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {mean_epoch_loss:.4f}')\n",
    "    \n",
    "    return train_loss\n",
    "\n",
    "\n",
    "# Test model\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss.append(loss.item())\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homeLocal/miniconda3/envs/cml/lib/python3.8/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484657607/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 0 train loss: 3.3328428110129207 test loss: 1.179932094085506\n",
      "Epoch 1 train loss: 1.1160664190728 test loss: 1.1269584498851515\n",
      "Epoch 2 train loss: 1.0736195973757579 test loss: 1.0635931244263297\n",
      "Epoch 3 train loss: 1.0486242275460642 test loss: 1.0667995878757106\n",
      "Epoch 4 train loss: 1.039539185162649 test loss: 1.058254580911463\n",
      "Epoch 5 train loss: 1.030193884644896 test loss: 1.0414488220272755\n",
      "Epoch 6 train loss: 1.0303288437786398 test loss: 1.039044724705087\n",
      "Epoch 7 train loss: 1.0199681858567928 test loss: 1.0474874307908726\n",
      "Epoch 8 train loss: 1.0207353704574296 test loss: 1.0400738793427733\n",
      "Epoch 9 train loss: 1.015044183207744 test loss: 1.0431303450948788\n",
      "Epoch 10 train loss: 1.0267308733101348 test loss: 1.0408739850339863\n",
      "Epoch 11 train loss: 1.0303115227383612 test loss: 1.0573459178755042\n",
      "Epoch 12 train loss: 1.0118360745208546 test loss: 1.0384970686733197\n",
      "Epoch 13 train loss: 1.017577848638577 test loss: 1.0288670865293477\n",
      "Epoch 14 train loss: 1.011976392602801 test loss: 1.060547931434059\n",
      "Epoch 15 train loss: 1.0144483039356047 test loss: 1.0382700249245294\n",
      "Epoch 16 train loss: 1.020491154807945 test loss: 1.0585298502940907\n",
      "Epoch 17 train loss: 1.0110956945684504 test loss: 1.0292504664510287\n",
      "Epoch 18 train loss: 1.0156381672196995 test loss: 1.0575477478079882\n",
      "Epoch 19 train loss: 1.0099033587603046 test loss: 1.0642838265104961\n",
      "Epoch 20 train loss: 1.0128727005055882 test loss: 1.0334825247882142\n",
      "Epoch 21 train loss: 1.0139757749082912 test loss: 1.0461759382540836\n",
      "Epoch 22 train loss: 1.0084771339720595 test loss: 1.0348477590595966\n",
      "Epoch 23 train loss: 1.0048220502866543 test loss: 1.0715282918210176\n",
      "Epoch 24 train loss: 1.0067186434809257 test loss: 1.0358924121639261\n",
      "Epoch 25 train loss: 1.0041810109303198 test loss: 1.0406128497162566\n",
      "Epoch 26 train loss: 1.0022010293356873 test loss: 1.0381609368291231\n",
      "Epoch 27 train loss: 1.0035865582592804 test loss: 1.0390592587483254\n",
      "Epoch 28 train loss: 1.004386365883651 test loss: 1.0350357127030139\n",
      "Epoch 29 train loss: 1.0040303505174346 test loss: 1.0344481353627912\n",
      "Epoch 30 train loss: 1.0077521958891809 test loss: 1.0756616700906656\n",
      "Epoch 31 train loss: 1.0064933011895894 test loss: 1.0312457252518412\n",
      "Epoch 32 train loss: 1.0072139182643993 test loss: 1.0427773711499062\n",
      "Epoch 33 train loss: 0.9957268444538542 test loss: 1.0240951820296373\n",
      "Epoch 34 train loss: 1.0023451933313883 test loss: 1.0355731861744104\n",
      "Epoch 35 train loss: 1.0026518651703993 test loss: 1.0277962465349777\n",
      "Epoch 36 train loss: 1.0003274353776022 test loss: 1.0615518221985412\n",
      "Epoch 37 train loss: 1.0002215893661734 test loss: 1.0857856040590927\n",
      "Epoch 38 train loss: 0.9983565883448585 test loss: 1.0333475012229587\n",
      "Epoch 39 train loss: 0.9978234506243616 test loss: 1.0369574258102754\n",
      "Epoch 40 train loss: 1.000317863546721 test loss: 1.0295230720030917\n",
      "Epoch 41 train loss: 1.000380229470996 test loss: 1.0227768658017142\n",
      "Epoch 42 train loss: 0.9970471424812939 test loss: 1.0476572651678233\n",
      "Epoch 43 train loss: 0.9945259253692019 test loss: 1.0392762560058704\n",
      "Epoch 44 train loss: 0.9941942417994264 test loss: 1.043886069143004\n",
      "Epoch 45 train loss: 0.9947433882663229 test loss: 1.0288284294598768\n",
      "Epoch 46 train loss: 0.9996399193561144 test loss: 1.063372609186679\n",
      "Epoch 47 train loss: 0.991634289412611 test loss: 1.0409387621974566\n",
      "Epoch 48 train loss: 0.998747825291059 test loss: 1.0496338316554015\n",
      "Epoch 49 train loss: 0.9928914206548113 test loss: 1.0383451093990224\n",
      "Epoch 50 train loss: 0.9917191074348937 test loss: 1.0346355811958166\n",
      "Epoch 51 train loss: 0.989292491362152 test loss: 1.0603501227431895\n",
      "Epoch 52 train loss: 0.9902175633595673 test loss: 1.0529182507388375\n",
      "Epoch 53 train loss: 0.9896454263933208 test loss: 1.0454793296490767\n",
      "Epoch 54 train loss: 0.9900459191057885 test loss: 1.0416522824008618\n",
      "Epoch 55 train loss: 0.9892107584060402 test loss: 1.0457600426536282\n",
      "Epoch 56 train loss: 0.9938470933844475 test loss: 1.0556501003649372\n",
      "Epoch 57 train loss: 0.9879467030311636 test loss: 1.0519250652436836\n",
      "Epoch 58 train loss: 0.9886860063966044 test loss: 1.0269356700064283\n",
      "Epoch 59 train loss: 0.9848957896791056 test loss: 1.0376788561280395\n",
      "Epoch 60 train loss: 0.9848172756006585 test loss: 1.0280564801162821\n",
      "Epoch 61 train loss: 0.9869896388800227 test loss: 1.0615278047133043\n",
      "Epoch 62 train loss: 0.9857380757710514 test loss: 1.0495005856281998\n",
      "Epoch 63 train loss: 0.9858990997606722 test loss: 1.0401759861827666\n",
      "Epoch 64 train loss: 0.9832165547171191 test loss: 1.0495854047653022\n",
      "Epoch 65 train loss: 0.9852959870059333 test loss: 1.0417342173160375\n",
      "Epoch 66 train loss: 0.9877379793942028 test loss: 1.0358709677275555\n",
      "Epoch 67 train loss: 0.9899641597539298 test loss: 1.0381862402958562\n",
      "Epoch 68 train loss: 0.98718344566702 test loss: 1.0324907537132604\n",
      "Epoch 69 train loss: 0.9795882380452112 test loss: 1.0437983026209046\n",
      "Epoch 70 train loss: 0.9838641570229791 test loss: 1.0259241098040432\n",
      "Epoch 71 train loss: 0.9829324783215154 test loss: 1.0380327291912441\n",
      "Epoch 72 train loss: 0.9860773527595866 test loss: 1.0359980955938588\n",
      "Epoch 73 train loss: 0.9819966450821312 test loss: 1.0371381878123689\n",
      "Epoch 74 train loss: 0.9829784767212051 test loss: 1.0435063893068817\n",
      "Epoch 75 train loss: 0.9837981194680261 test loss: 1.0491810381738602\n",
      "Epoch 76 train loss: 0.9811995365293393 test loss: 1.0666905809307794\n",
      "Epoch 77 train loss: 0.9761225177073324 test loss: 1.0523690130943586\n",
      "Epoch 78 train loss: 0.9802000077604401 test loss: 1.0460456738963568\n",
      "Epoch 79 train loss: 0.9809772207539177 test loss: 1.0495289532827061\n",
      "Epoch 80 train loss: 0.9803062242314858 test loss: 1.0610447492275732\n",
      "Epoch 81 train loss: 0.978081372380081 test loss: 1.0309036040372692\n",
      "Epoch 82 train loss: 0.9802066953354942 test loss: 1.0516456536940115\n",
      "Epoch 83 train loss: 0.9784268312139164 test loss: 1.0373709839728318\n",
      "Epoch 84 train loss: 0.9748649081941058 test loss: 1.0505018331195364\n",
      "Epoch 85 train loss: 0.9775136065543508 test loss: 1.0410347106634055\n",
      "Epoch 86 train loss: 0.9714117701719951 test loss: 1.0532914451292368\n",
      "Epoch 87 train loss: 0.9828878549759016 test loss: 1.0481196144113185\n",
      "Epoch 88 train loss: 0.9758939571008655 test loss: 1.0438581640543023\n",
      "Epoch 89 train loss: 0.9719027468500921 test loss: 1.05191241970422\n",
      "Epoch 90 train loss: 0.9763279536950636 test loss: 1.0352255999709246\n",
      "Epoch 91 train loss: 0.9798000865187071 test loss: 1.0307467564576078\n",
      "Epoch 92 train loss: 0.9740355456776637 test loss: 1.045278191325746\n",
      "Epoch 93 train loss: 0.9731184305158379 test loss: 1.0416147166208676\n",
      "Epoch 94 train loss: 0.9849450090503846 test loss: 1.0457627677911687\n",
      "Epoch 95 train loss: 0.9730295545484241 test loss: 1.0444126733272294\n",
      "Epoch 96 train loss: 0.9729909482896894 test loss: 1.0449542176312503\n",
      "Epoch 97 train loss: 0.9717508326804999 test loss: 1.0506512037660405\n",
      "Epoch 98 train loss: 0.9759942677448616 test loss: 1.0401626162481385\n",
      "Epoch 99 train loss: 0.9690759731119944 test loss: 1.0340776134573175\n",
      "Epoch 100 train loss: 0.9707207590875488 test loss: 1.05358542669567\n",
      "Epoch 101 train loss: 0.9691118049802706 test loss: 1.0417987259258128\n",
      "Epoch 102 train loss: 0.9737603403008961 test loss: 1.046220425423213\n",
      "Epoch 103 train loss: 0.9706194483872929 test loss: 1.062951725439091\n",
      "Epoch 104 train loss: 0.9713223131442981 test loss: 1.047703849867447\n",
      "Epoch 105 train loss: 0.9681728016637211 test loss: 1.0514690536037765\n",
      "Epoch 106 train loss: 0.9672407410599876 test loss: 1.0409806384329234\n",
      "Epoch 107 train loss: 0.9675651498082125 test loss: 1.0452799898767702\n",
      "Epoch 108 train loss: 0.9676115733521055 test loss: 1.0737583743915824\n",
      "Epoch 109 train loss: 0.9720670039543585 test loss: 1.0676247854010164\n",
      "Epoch 110 train loss: 0.9671708832459237 test loss: 1.0679728145139329\n",
      "Epoch 111 train loss: 0.9688535015721297 test loss: 1.0509973023351806\n",
      "Epoch 112 train loss: 0.9665389869261161 test loss: 1.0529908285776222\n",
      "Epoch 113 train loss: 0.9662404929644045 test loss: 1.0361994491616047\n",
      "Epoch 114 train loss: 0.9641286676699112 test loss: 1.0431281840816962\n",
      "Epoch 115 train loss: 0.9679798277482977 test loss: 1.0506750291664289\n",
      "Epoch 116 train loss: 0.9653131088729279 test loss: 1.0607067886184556\n",
      "Epoch 117 train loss: 0.9721398857618365 test loss: 1.0422866416110597\n",
      "Epoch 118 train loss: 0.9610505866777885 test loss: 1.057850531061307\n",
      "Epoch 119 train loss: 0.9629462505988003 test loss: 1.068486526926156\n",
      "Epoch 120 train loss: 0.9615645388790262 test loss: 1.0553528038280917\n",
      "Epoch 121 train loss: 0.9654317757952632 test loss: 1.081591921121674\n",
      "Epoch 122 train loss: 0.9651949158877912 test loss: 1.0612800566420901\n",
      "Epoch 123 train loss: 0.9600045476025217 test loss: 1.0555292081936472\n",
      "Epoch 124 train loss: 0.9601417067617876 test loss: 1.0515943730685602\n",
      "Epoch 125 train loss: 0.9622623089526807 test loss: 1.0588717221444757\n",
      "Epoch 126 train loss: 0.9580932823865231 test loss: 1.0529145363520396\n",
      "Epoch 127 train loss: 0.9607165441324443 test loss: 1.051701912393409\n",
      "Epoch 128 train loss: 0.965772034313613 test loss: 1.0527348346610932\n",
      "Epoch 129 train loss: 0.9566992245759836 test loss: 1.065106362182549\n",
      "Epoch 130 train loss: 0.9659764519799425 test loss: 1.0567642083693305\n",
      "Epoch 131 train loss: 0.9632880171809715 test loss: 1.049350654841204\n",
      "Epoch 132 train loss: 0.9556593367903428 test loss: 1.0377268648897795\n",
      "Epoch 133 train loss: 0.9574516584779009 test loss: 1.0689234912186905\n",
      "Epoch 134 train loss: 0.9586974074223928 test loss: 1.0627816193417128\n",
      "Epoch 135 train loss: 0.9641199599767095 test loss: 1.0551015614630552\n",
      "Epoch 136 train loss: 0.9576344492654518 test loss: 1.0611362353585925\n",
      "Epoch 137 train loss: 0.9662072680030076 test loss: 1.0869539822238798\n",
      "Epoch 138 train loss: 0.9641061239729475 test loss: 1.0630778425614962\n",
      "Epoch 139 train loss: 0.9526416728282026 test loss: 1.0669608820461072\n",
      "Epoch 140 train loss: 0.9537207593547348 test loss: 1.0567724862555452\n",
      "Epoch 141 train loss: 0.9520116930325604 test loss: 1.056927193476395\n",
      "Epoch 142 train loss: 0.9604936479431194 test loss: 1.0768707937269235\n",
      "Epoch 143 train loss: 0.9498817620090535 test loss: 1.064613421160216\n",
      "Epoch 144 train loss: 0.9486018180799884 test loss: 1.0660173077376771\n",
      "Epoch 145 train loss: 0.9577808905483951 test loss: 1.0547734550278607\n",
      "Epoch 146 train loss: 0.954898710560799 test loss: 1.044284091489006\n",
      "Epoch 147 train loss: 0.9480236927005438 test loss: 1.0667181719898968\n",
      "Epoch 148 train loss: 0.9506735205553507 test loss: 1.05682888871055\n",
      "Epoch 149 train loss: 0.9517427783158717 test loss: 1.09086680735889\n",
      "Epoch 150 train loss: 0.9490483233834529 test loss: 1.0730708159584463\n",
      "Epoch 151 train loss: 0.9512009919216073 test loss: 1.0656580667779056\n",
      "Epoch 152 train loss: 0.952785556491422 test loss: 1.0545370216511951\n",
      "Epoch 153 train loss: 0.9501242620959438 test loss: 1.0810396563292897\n",
      "Epoch 154 train loss: 0.9450211500431053 test loss: 1.0592451298217558\n",
      "Epoch 155 train loss: 0.9428373954496456 test loss: 1.0708050736576005\n",
      "Epoch 156 train loss: 0.9456597222713944 test loss: 1.0644346612104152\n",
      "Epoch 157 train loss: 0.946787123881715 test loss: 1.0797735360410048\n",
      "Epoch 158 train loss: 0.9507981867595294 test loss: 1.0491279701243776\n",
      "Epoch 159 train loss: 0.9478981582129107 test loss: 1.090249462429271\n",
      "Epoch 160 train loss: 0.944094149268267 test loss: 1.0718540007692128\n",
      "Epoch 161 train loss: 0.9442607988580737 test loss: 1.063433398474372\n",
      "Epoch 162 train loss: 0.9431141602883397 test loss: 1.0706406768871761\n",
      "Epoch 163 train loss: 0.9419036974259063 test loss: 1.0717401621268308\n",
      "Epoch 164 train loss: 0.9426028444844059 test loss: 1.0848152032619864\n",
      "Epoch 165 train loss: 0.9413201872155763 test loss: 1.05154888057022\n",
      "Epoch 166 train loss: 0.9415009357252003 test loss: 1.0584419417517101\n",
      "Epoch 167 train loss: 0.9426727241389594 test loss: 1.0810017233932638\n",
      "Epoch 168 train loss: 0.9466275542299188 test loss: 1.0759241710166505\n",
      "Epoch 169 train loss: 0.941713922174487 test loss: 1.0616465668604038\n",
      "Epoch 170 train loss: 0.9383267499988345 test loss: 1.0728562268250192\n",
      "Epoch 171 train loss: 0.9327799233780957 test loss: 1.0799248156567702\n",
      "Epoch 172 train loss: 0.938454463520634 test loss: 1.0819935685371895\n",
      "Epoch 173 train loss: 0.9354615776350546 test loss: 1.0810366328188945\n",
      "Epoch 174 train loss: 0.9338601836684305 test loss: 1.083502989812406\n",
      "Epoch 175 train loss: 0.9307439161341304 test loss: 1.0763944000037045\n",
      "Epoch 176 train loss: 0.9360621145883846 test loss: 1.078622540518603\n",
      "Epoch 177 train loss: 0.9336475862688665 test loss: 1.0847261882492463\n",
      "Epoch 178 train loss: 0.9323816842805213 test loss: 1.1074665423420529\n",
      "Epoch 179 train loss: 0.9346176963633374 test loss: 1.075124598717435\n",
      "Epoch 180 train loss: 0.9305520998998428 test loss: 1.0799308507114942\n",
      "Epoch 181 train loss: 0.9313251886294368 test loss: 1.0829782979864566\n",
      "Epoch 182 train loss: 0.9297342191290879 test loss: 1.102987358182589\n",
      "Epoch 183 train loss: 0.9294574415972108 test loss: 1.090351609142298\n",
      "Epoch 184 train loss: 0.9287650883007066 test loss: 1.0766059834455337\n",
      "Epoch 185 train loss: 0.9280344474648389 test loss: 1.085496898546026\n",
      "Epoch 186 train loss: 0.9253387141202845 test loss: 1.0735905585565195\n",
      "Epoch 187 train loss: 0.928198056649842 test loss: 1.0874632251346648\n",
      "Epoch 188 train loss: 0.9261812790485674 test loss: 1.0890726776796613\n",
      "Epoch 189 train loss: 0.9307602602378074 test loss: 1.073800179972176\n",
      "Epoch 190 train loss: 0.9236276063230249 test loss: 1.0830990175342192\n",
      "Epoch 191 train loss: 0.9261688746531169 test loss: 1.0962598548620095\n",
      "Epoch 192 train loss: 0.9239623117761515 test loss: 1.0936278282869263\n",
      "Epoch 193 train loss: 0.9278177633415627 test loss: 1.087298063579562\n",
      "Epoch 194 train loss: 0.9236228174690363 test loss: 1.090640747683169\n",
      "Epoch 195 train loss: 0.9200830516423553 test loss: 1.0960197666887357\n",
      "Epoch 196 train loss: 0.9193265891414296 test loss: 1.1029869904262537\n",
      "Epoch 197 train loss: 0.9218156570541843 test loss: 1.0987502232530413\n",
      "Epoch 198 train loss: 0.9224401498963676 test loss: 1.0854637477137314\n",
      "Epoch 199 train loss: 0.9156589011662903 test loss: 1.0908456486260598\n",
      "Epoch 200 train loss: 0.9197977460225806 test loss: 1.0922132351715357\n",
      "Epoch 201 train loss: 0.9163013661532328 test loss: 1.1074214949866537\n",
      "Epoch 202 train loss: 0.9198788001784457 test loss: 1.0972640348659524\n",
      "Epoch 203 train loss: 0.9149988977099333 test loss: 1.1034910615520057\n",
      "Epoch 204 train loss: 0.9234890357220945 test loss: 1.097468255521851\n",
      "Epoch 205 train loss: 0.9165359735296355 test loss: 1.1176251330503497\n",
      "Epoch 206 train loss: 0.9179776689100859 test loss: 1.1068481741905318\n",
      "Epoch 207 train loss: 0.914650944300712 test loss: 1.082569983310439\n",
      "Epoch 208 train loss: 0.9154231043236735 test loss: 1.096657739787426\n",
      "Epoch 209 train loss: 0.9141591999001275 test loss: 1.0947471056955744\n",
      "Epoch 210 train loss: 0.9130985288678292 test loss: 1.1097195630713115\n",
      "Epoch 211 train loss: 0.9129468479034724 test loss: 1.0947755964304806\n",
      "Epoch 212 train loss: 0.9092676009276106 test loss: 1.1015483924598\n",
      "Epoch 213 train loss: 0.9088523730518366 test loss: 1.0816964153329125\n",
      "Epoch 214 train loss: 0.9150471129875392 test loss: 1.0933991760203012\n",
      "Epoch 215 train loss: 0.9141230143436275 test loss: 1.110790233600115\n",
      "Epoch 216 train loss: 0.9116994774267536 test loss: 1.1081765711686384\n",
      "Epoch 217 train loss: 0.9081423423937519 test loss: 1.1033519448290585\n",
      "Epoch 218 train loss: 0.9085088235437939 test loss: 1.107955776746162\n",
      "Epoch 219 train loss: 0.9061066796769617 test loss: 1.1208024240118772\n",
      "Epoch 220 train loss: 0.9098878890560241 test loss: 1.1004875809956505\n",
      "Epoch 221 train loss: 0.9044254287931685 test loss: 1.1068675555054002\n",
      "Epoch 222 train loss: 0.9067561551989138 test loss: 1.1176067079311518\n",
      "Epoch 223 train loss: 0.9078598474166624 test loss: 1.1122469742090155\n",
      "Epoch 224 train loss: 0.9116791088346203 test loss: 1.1111895419673543\n",
      "Epoch 225 train loss: 0.9062854999553654 test loss: 1.115392397269376\n",
      "Epoch 226 train loss: 0.9069079602905891 test loss: 1.1187697862055022\n",
      "Epoch 227 train loss: 0.9108659834577996 test loss: 1.0952807984588344\n",
      "Epoch 228 train loss: 0.9130951522615254 test loss: 1.164963274687976\n",
      "Epoch 229 train loss: 0.9082198646876974 test loss: 1.0995252476110162\n",
      "Epoch 230 train loss: 0.898675863820507 test loss: 1.0871436718796472\n",
      "Epoch 231 train loss: 0.9024327818475566 test loss: 1.1285568080339554\n",
      "Epoch 232 train loss: 0.9009587932185562 test loss: 1.093774348234161\n",
      "Epoch 233 train loss: 0.8984343225739088 test loss: 1.0982497231641415\n",
      "Epoch 234 train loss: 0.8962610690309571 test loss: 1.0977775890487922\n",
      "Epoch 235 train loss: 0.8948151028723442 test loss: 1.1037730399342842\n",
      "Epoch 236 train loss: 0.896936285805407 test loss: 1.1026097827253705\n",
      "Epoch 237 train loss: 0.8952722980736558 test loss: 1.1132717629470434\n",
      "Epoch 238 train loss: 0.8977594213907379 test loss: 1.1049113381338609\n",
      "Epoch 239 train loss: 0.8962459534342109 test loss: 1.112656526548962\n",
      "Epoch 240 train loss: 0.8941180933592315 test loss: 1.1168803075673566\n",
      "Epoch 241 train loss: 0.8898058141627101 test loss: 1.126143207670542\n",
      "Epoch 242 train loss: 0.8949522434890956 test loss: 1.1348792207256917\n",
      "Epoch 243 train loss: 0.8964197934340069 test loss: 1.1469265425727726\n",
      "Epoch 244 train loss: 0.8961649205760867 test loss: 1.1385580822505594\n",
      "Epoch 245 train loss: 0.8913772612865805 test loss: 1.1259680095557534\n",
      "Epoch 246 train loss: 0.8911574772610494 test loss: 1.1610439789271538\n",
      "Epoch 247 train loss: 0.8968277381769518 test loss: 1.1206548629161275\n",
      "Epoch 248 train loss: 0.8834803665987804 test loss: 1.1106739952463092\n",
      "Epoch 249 train loss: 0.8890459857145383 test loss: 1.1188446979211475\n",
      "Epoch 250 train loss: 0.8855186889103368 test loss: 1.1450264702798325\n",
      "Epoch 251 train loss: 0.8883129048042054 test loss: 1.1096546771571827\n",
      "Epoch 252 train loss: 0.8864429410984664 test loss: 1.1310085294391452\n",
      "Epoch 253 train loss: 0.8844344821704478 test loss: 1.1290649875617007\n",
      "Epoch 254 train loss: 0.882551829269617 test loss: 1.131400659413773\n",
      "Epoch 255 train loss: 0.887550594696373 test loss: 1.1017616559033132\n",
      "Epoch 256 train loss: 0.8835679271050192 test loss: 1.11571992922672\n",
      "Epoch 257 train loss: 0.8851413332804116 test loss: 1.110660415446743\n",
      "Epoch 258 train loss: 0.885571822996021 test loss: 1.1245284664061512\n",
      "Epoch 259 train loss: 0.8832500770335784 test loss: 1.1267975279036384\n",
      "Epoch 260 train loss: 0.8796990595010359 test loss: 1.1278619061326878\n",
      "Epoch 261 train loss: 0.8804108750137998 test loss: 1.1026597257624915\n",
      "Epoch 262 train loss: 0.8864840667825137 test loss: 1.1202444340003281\n",
      "Epoch 263 train loss: 0.8776334521217645 test loss: 1.1507716097701937\n",
      "Epoch 264 train loss: 0.8790786311121925 test loss: 1.144795063416407\n",
      "Epoch 265 train loss: 0.8772357942594433 test loss: 1.1278326896039426\n",
      "Epoch 266 train loss: 0.880010572303701 test loss: 1.1370457159882008\n",
      "Epoch 267 train loss: 0.8783104938514513 test loss: 1.1288740578423584\n",
      "Epoch 268 train loss: 0.8761228471845858 test loss: 1.1360492116621836\n",
      "Epoch 269 train loss: 0.8837985165070411 test loss: 1.1276093508571599\n",
      "Epoch 270 train loss: 0.8742069100193808 test loss: 1.1422157692298356\n",
      "Epoch 271 train loss: 0.877452362874718 test loss: 1.1431648885096795\n",
      "Epoch 272 train loss: 0.8816442798101093 test loss: 1.1897437971212321\n",
      "Epoch 273 train loss: 0.8728735982866909 test loss: 1.1594735668539147\n",
      "Epoch 274 train loss: 0.8785880787545542 test loss: 1.1234770062105128\n",
      "Epoch 275 train loss: 0.8710593105942871 test loss: 1.1650705794471907\n",
      "Epoch 276 train loss: 0.8759484093529242 test loss: 1.1471478348905415\n",
      "Epoch 277 train loss: 0.8709693117821137 test loss: 1.1337488168623149\n",
      "Epoch 278 train loss: 0.8685298651206642 test loss: 1.1361915082677154\n",
      "Epoch 279 train loss: 0.8703888081492265 test loss: 1.1325279331728484\n",
      "Epoch 280 train loss: 0.870731785610778 test loss: 1.1613318920691538\n",
      "Epoch 281 train loss: 0.8735742951544786 test loss: 1.152754587176362\n",
      "Epoch 282 train loss: 0.8730820796594468 test loss: 1.1764439632055943\n",
      "Epoch 283 train loss: 0.8685716083873147 test loss: 1.1175713693209963\n",
      "Epoch 284 train loss: 0.865762895362148 test loss: 1.153617439847859\n",
      "Epoch 285 train loss: 0.867336485802403 test loss: 1.1581157206325\n",
      "Epoch 286 train loss: 0.8664346936338764 test loss: 1.148106970690945\n",
      "Epoch 287 train loss: 0.8685613552221271 test loss: 1.1625159497783784\n",
      "Epoch 288 train loss: 0.8700274850836518 test loss: 1.136258259238896\n",
      "Epoch 289 train loss: 0.8674207646204629 test loss: 1.191871129625535\n",
      "Epoch 290 train loss: 0.8638696092083825 test loss: 1.1632984003310562\n",
      "Epoch 291 train loss: 0.8599835292141713 test loss: 1.1479652714821076\n",
      "Epoch 292 train loss: 0.8673592387137682 test loss: 1.1576441655727852\n",
      "Epoch 293 train loss: 0.8645175199065812 test loss: 1.1439934791554394\n",
      "Epoch 294 train loss: 0.8719820175301326 test loss: 1.1538712891476055\n",
      "Epoch 295 train loss: 0.8630872953667751 test loss: 1.1574427728998797\n",
      "Epoch 296 train loss: 0.8546032504710155 test loss: 1.1672359282136036\n",
      "Epoch 297 train loss: 0.8597322366203597 test loss: 1.199561055073652\n",
      "Epoch 298 train loss: 0.861316955603283 test loss: 1.158253385391086\n",
      "Epoch 299 train loss: 0.8616736882778981 test loss: 1.1364992047294435\n",
      "Epoch 300 train loss: 0.8565499910244366 test loss: 1.1628613967327055\n",
      "Epoch 301 train loss: 0.8613049979192501 test loss: 1.1589720345154781\n",
      "Epoch 302 train loss: 0.8547056668679969 test loss: 1.1654430100105047\n",
      "Epoch 303 train loss: 0.8574447073575475 test loss: 1.1443765218551847\n",
      "Epoch 304 train loss: 0.8628951466694462 test loss: 1.1464930510440428\n",
      "Epoch 305 train loss: 0.8570610949514157 test loss: 1.1339021201250516\n",
      "Epoch 306 train loss: 0.8539295496348825 test loss: 1.1732543373721256\n",
      "Epoch 307 train loss: 0.8614856410483275 test loss: 1.1525211084674498\n",
      "Epoch 308 train loss: 0.857383496174322 test loss: 1.1444030946911417\n",
      "Epoch 309 train loss: 0.8546584267522507 test loss: 1.1638945713366842\n",
      "Epoch 310 train loss: 0.8530929187466927 test loss: 1.1839876894897723\n",
      "Epoch 311 train loss: 0.8490334285260102 test loss: 1.1619486903508038\n",
      "Epoch 312 train loss: 0.8551235706416406 test loss: 1.1753992275215999\n",
      "Epoch 313 train loss: 0.8517245065518309 test loss: 1.1757665572946225\n",
      "Epoch 314 train loss: 0.849017555559627 test loss: 1.1913771201972012\n",
      "Epoch 315 train loss: 0.8509442285088932 test loss: 1.1463573999236516\n",
      "Epoch 316 train loss: 0.8478832802330202 test loss: 1.182111302721903\n",
      "Epoch 317 train loss: 0.8480594025187265 test loss: 1.1635148276603977\n",
      "Epoch 318 train loss: 0.844618922409736 test loss: 1.1581882350808217\n",
      "Epoch 319 train loss: 0.8458485599984656 test loss: 1.1786668207379107\n",
      "Epoch 320 train loss: 0.8521713180456892 test loss: 1.1500529585276613\n",
      "Epoch 321 train loss: 0.8440437052023388 test loss: 1.156120888246684\n",
      "Epoch 322 train loss: 0.8506618472198273 test loss: 1.1690281077119589\n",
      "Epoch 323 train loss: 0.8487056469574668 test loss: 1.1839339698306577\n",
      "Epoch 324 train loss: 0.8479589223958183 test loss: 1.1665180158689044\n",
      "Epoch 325 train loss: 0.8426910912455581 test loss: 1.1824032288046857\n",
      "Epoch 326 train loss: 0.8409145074886042 test loss: 1.184892209632412\n",
      "Epoch 327 train loss: 0.841897018181048 test loss: 1.198776029553318\n",
      "Epoch 328 train loss: 0.8435429829563788 test loss: 1.1758323090533587\n",
      "Epoch 329 train loss: 0.8422241068944304 test loss: 1.2099824325488182\n",
      "Epoch 330 train loss: 0.8500927283611116 test loss: 1.1788530259950005\n",
      "Epoch 331 train loss: 0.8451194391497742 test loss: 1.1824149330939733\n",
      "Epoch 332 train loss: 0.8390115241300939 test loss: 1.1831733665584823\n",
      "Epoch 333 train loss: 0.8430038290353209 test loss: 1.1663545023374642\n",
      "Epoch 334 train loss: 0.8403261328153101 test loss: 1.1754321569258108\n",
      "Epoch 335 train loss: 0.8342986629302314 test loss: 1.1748451394876473\n",
      "Epoch 336 train loss: 0.8347501543524904 test loss: 1.187346794722842\n",
      "Epoch 337 train loss: 0.8358397165828209 test loss: 1.177676806776687\n",
      "Epoch 338 train loss: 0.836251359068517 test loss: 1.1780281184047374\n",
      "Epoch 339 train loss: 0.8367439188084829 test loss: 1.1872981057555871\n",
      "Epoch 340 train loss: 0.8390232918454612 test loss: 1.1760018781765544\n",
      "Epoch 341 train loss: 0.8345929406730045 test loss: 1.1784682298615814\n",
      "Epoch 342 train loss: 0.8335501686807386 test loss: 1.1934075033683924\n",
      "Epoch 343 train loss: 0.8305928228728934 test loss: 1.1825562503573703\n",
      "Epoch 344 train loss: 0.8339184390137281 test loss: 1.1821139298404875\n",
      "Epoch 345 train loss: 0.8308870976856096 test loss: 1.1993047229127827\n",
      "Epoch 346 train loss: 0.8288692073913865 test loss: 1.1886423954667344\n",
      "Epoch 347 train loss: 0.8341941124241883 test loss: 1.1901870268525798\n",
      "Epoch 348 train loss: 0.8282059968044837 test loss: 1.218886611481588\n",
      "Epoch 349 train loss: 0.8357985772478916 test loss: 1.1892423133454852\n",
      "Epoch 350 train loss: 0.8339179271783186 test loss: 1.1909129939253278\n",
      "Epoch 351 train loss: 0.8333121244414505 test loss: 1.1894777600797748\n",
      "Epoch 352 train loss: 0.8302051026932086 test loss: 1.195878566432506\n",
      "Epoch 353 train loss: 0.8294178965027985 test loss: 1.188538596017487\n",
      "Epoch 354 train loss: 0.8281257630316832 test loss: 1.232012167705837\n",
      "Epoch 355 train loss: 0.8298114741436537 test loss: 1.2058784889566336\n",
      "Epoch 356 train loss: 0.8272863418114561 test loss: 1.1999942002543897\n",
      "Epoch 357 train loss: 0.8280289356894035 test loss: 1.1744668765282742\n",
      "Epoch 358 train loss: 0.8235742619499391 test loss: 1.2149740653171108\n",
      "Epoch 359 train loss: 0.8290951503462707 test loss: 1.2152755289084292\n",
      "Epoch 360 train loss: 0.828462035118519 test loss: 1.19797910660906\n",
      "Epoch 361 train loss: 0.8248860793105957 test loss: 1.2020520233313357\n",
      "Epoch 362 train loss: 0.8302194466592081 test loss: 1.1871376831390585\n",
      "Epoch 363 train loss: 0.826978869604299 test loss: 1.2213599580446746\n",
      "Epoch 364 train loss: 0.8251115202936383 test loss: 1.1968955010519637\n",
      "Epoch 365 train loss: 0.8216872607726484 test loss: 1.2103443900475837\n",
      "Epoch 366 train loss: 0.8202715599387032 test loss: 1.2242348769902534\n",
      "Epoch 367 train loss: 0.8268842393942738 test loss: 1.2138410506503614\n",
      "Epoch 368 train loss: 0.8235590101425233 test loss: 1.1968345136259881\n",
      "Epoch 369 train loss: 0.8205676810707093 test loss: 1.1984937069239303\n",
      "Epoch 370 train loss: 0.8258385371269391 test loss: 1.2227622780878007\n",
      "Epoch 371 train loss: 0.8192136976464228 test loss: 1.1941651209200472\n",
      "Epoch 372 train loss: 0.8187646252868265 test loss: 1.2316606246574937\n",
      "Epoch 373 train loss: 0.817982617932724 test loss: 1.2023870517964859\n",
      "Epoch 374 train loss: 0.8190867665500378 test loss: 1.2048565870373769\n",
      "Epoch 375 train loss: 0.8197566212015998 test loss: 1.2018048316431829\n",
      "Epoch 376 train loss: 0.815018860969884 test loss: 1.2083287384875545\n",
      "Epoch 377 train loss: 0.8214924258528566 test loss: 1.225001062149256\n",
      "Epoch 378 train loss: 0.8180889725203462 test loss: 1.2223040816910304\n",
      "Epoch 379 train loss: 0.8156410646492496 test loss: 1.2216132642349091\n",
      "Epoch 380 train loss: 0.8200852358697592 test loss: 1.2130835322705917\n",
      "Epoch 381 train loss: 0.807321532831996 test loss: 1.2169401462837564\n",
      "Epoch 382 train loss: 0.8127819591713458 test loss: 1.2200179280360235\n",
      "Epoch 383 train loss: 0.8108948138793415 test loss: 1.2119319802923378\n",
      "Epoch 384 train loss: 0.8135720815554052 test loss: 1.2121771858674841\n",
      "Epoch 385 train loss: 0.8140606551853943 test loss: 1.2162280695345546\n",
      "Epoch 386 train loss: 0.8163755199038591 test loss: 1.182845007670867\n",
      "Epoch 387 train loss: 0.8131433194757516 test loss: 1.190293034896047\n",
      "Epoch 388 train loss: 0.8128163244185233 test loss: 1.2165762100658921\n",
      "Epoch 389 train loss: 0.8157429115388907 test loss: 1.213008067289207\n",
      "Epoch 390 train loss: 0.8089583791639946 test loss: 1.206049227286372\n",
      "Epoch 391 train loss: 0.8093836068313005 test loss: 1.2337954015223844\n",
      "Epoch 392 train loss: 0.8096239320861357 test loss: 1.2105861387349321\n",
      "Epoch 393 train loss: 0.8079128528019015 test loss: 1.2300181646973618\n",
      "Epoch 394 train loss: 0.8050110329991617 test loss: 1.2244028978444366\n",
      "Epoch 395 train loss: 0.8061832015393243 test loss: 1.2317745026645124\n",
      "Epoch 396 train loss: 0.8136570118128853 test loss: 1.2155964405646358\n",
      "Epoch 397 train loss: 0.8106861302911537 test loss: 1.2029469063749298\n",
      "Epoch 398 train loss: 0.8115924848777898 test loss: 1.191812116689398\n",
      "Epoch 399 train loss: 0.8063158789996934 test loss: 1.2019524497188763\n",
      "Epoch 400 train loss: 0.7984378510831371 test loss: 1.2252725162422793\n",
      "Epoch 401 train loss: 0.8085863356431753 test loss: 1.2030973384185595\n",
      "Epoch 402 train loss: 0.8025054572889584 test loss: 1.23808427800269\n",
      "Epoch 403 train loss: 0.8022546310943227 test loss: 1.2239264543689132\n",
      "Epoch 404 train loss: 0.8044052890210174 test loss: 1.2198932562670508\n",
      "Epoch 405 train loss: 0.8080165474222498 test loss: 1.2244813431800488\n",
      "Epoch 406 train loss: 0.7964859827795634 test loss: 1.2474069575568734\n",
      "Epoch 407 train loss: 0.8033081459798226 test loss: 1.2249540828462737\n",
      "Epoch 408 train loss: 0.7961395507205089 test loss: 1.216323702296396\n",
      "Epoch 409 train loss: 0.802187385622362 test loss: 1.251276837024408\n",
      "Epoch 410 train loss: 0.7984735252469088 test loss: 1.2119968047229532\n",
      "Epoch 411 train loss: 0.8041643248265911 test loss: 1.232982487751798\n",
      "Epoch 412 train loss: 0.8023111816036516 test loss: 1.2212714090619974\n",
      "Epoch 413 train loss: 0.8024543974974595 test loss: 1.2149265427632745\n",
      "Epoch 414 train loss: 0.7965539181651146 test loss: 1.2489281515346689\n",
      "Epoch 415 train loss: 0.7949178855755182 test loss: 1.2189255515191095\n",
      "Epoch 416 train loss: 0.7999621770043008 test loss: 1.2286621955704233\n",
      "Epoch 417 train loss: 0.7935166810312416 test loss: 1.2368108737224688\n",
      "Epoch 418 train loss: 0.7940356173926049 test loss: 1.2218408353791113\n",
      "Epoch 419 train loss: 0.793542651040922 test loss: 1.2460810336470385\n",
      "Epoch 420 train loss: 0.7940491726062724 test loss: 1.2270248243667867\n",
      "Epoch 421 train loss: 0.7899273293848627 test loss: 1.221716553795743\n",
      "Epoch 422 train loss: 0.7918293383651456 test loss: 1.2496906964892247\n",
      "Epoch 423 train loss: 0.793598182854852 test loss: 1.2302205473950596\n",
      "Epoch 424 train loss: 0.7944033536201005 test loss: 1.22259788867557\n",
      "Epoch 425 train loss: 0.7950442098794894 test loss: 1.2151723518271584\n",
      "Epoch 426 train loss: 0.7931788494894783 test loss: 1.2285822378294975\n",
      "Epoch 427 train loss: 0.797192575437861 test loss: 1.2530857023114694\n",
      "Epoch 428 train loss: 0.7920201649337815 test loss: 1.243042878307887\n",
      "Epoch 429 train loss: 0.7911192518751133 test loss: 1.2875113965129033\n",
      "Epoch 430 train loss: 0.7868288300764651 test loss: 1.2789472309342766\n",
      "Epoch 431 train loss: 0.7888941124334862 test loss: 1.2656104983702745\n",
      "Epoch 432 train loss: 0.7961898379824471 test loss: 1.2153508097703503\n",
      "Epoch 433 train loss: 0.7932381837347349 test loss: 1.2164484849911723\n",
      "Epoch 434 train loss: 0.7919849544464117 test loss: 1.2487138035117442\n",
      "Epoch 435 train loss: 0.7882301070043234 test loss: 1.2563160964118563\n",
      "Epoch 436 train loss: 0.7917679058100632 test loss: 1.2369991615478937\n",
      "Epoch 437 train loss: 0.7909142874057511 test loss: 1.266846244359492\n",
      "Epoch 438 train loss: 0.7890344945497154 test loss: 1.2343240719240138\n",
      "Epoch 439 train loss: 0.787303312066986 test loss: 1.2658707810367253\n",
      "Epoch 440 train loss: 0.7864215670096187 test loss: 1.2493609018902363\n",
      "Epoch 441 train loss: 0.7831444534320899 test loss: 1.221484743336478\n",
      "Epoch 442 train loss: 0.7856517890836144 test loss: 1.2178064759638025\n",
      "Epoch 443 train loss: 0.7864150101754422 test loss: 1.231087220471041\n",
      "Epoch 444 train loss: 0.7808032769153853 test loss: 1.2375858711545131\n",
      "Epoch 445 train loss: 0.7829209440130975 test loss: 1.2184974973112215\n",
      "Epoch 446 train loss: 0.7791376629827841 test loss: 1.2503644791618713\n",
      "Epoch 447 train loss: 0.7764767063712498 test loss: 1.2480592362109926\n",
      "Epoch 448 train loss: 0.7790895332352942 test loss: 1.243531976880508\n",
      "Epoch 449 train loss: 0.782381043963785 test loss: 1.2242047270754584\n",
      "Epoch 450 train loss: 0.7786183408592051 test loss: 1.255443480567417\n",
      "Epoch 451 train loss: 0.7851093746613286 test loss: 1.2670208806365397\n",
      "Epoch 452 train loss: 0.7816529421447016 test loss: 1.2403591603793822\n",
      "Epoch 453 train loss: 0.7860115712996671 test loss: 1.2399154460018749\n",
      "Epoch 454 train loss: 0.7811599289908078 test loss: 1.2741756683321857\n",
      "Epoch 455 train loss: 0.777547651705303 test loss: 1.2267881998656662\n",
      "Epoch 456 train loss: 0.7827121153931251 test loss: 1.2539695321656774\n",
      "Epoch 457 train loss: 0.7793519077053142 test loss: 1.2569914288170736\n",
      "Epoch 458 train loss: 0.7750908427042188 test loss: 1.2531766836818736\n",
      "Epoch 459 train loss: 0.7766483000082172 test loss: 1.2301811756278753\n",
      "Epoch 460 train loss: 0.7778273774184925 test loss: 1.2538015814538919\n",
      "Epoch 461 train loss: 0.7747431433953652 test loss: 1.2639065099657363\n",
      "Epoch 462 train loss: 0.774504443103039 test loss: 1.2446524583528\n",
      "Epoch 463 train loss: 0.7764253665240565 test loss: 1.2386417225088953\n",
      "Epoch 464 train loss: 0.7758031063518728 test loss: 1.2292746512284394\n",
      "Epoch 465 train loss: 0.7759260159428562 test loss: 1.2523242632572344\n",
      "Epoch 466 train loss: 0.7723301365873088 test loss: 1.2794401845338987\n",
      "Epoch 467 train loss: 0.775441779456574 test loss: 1.2595488716637133\n",
      "Epoch 468 train loss: 0.7754530013319879 test loss: 1.2405822938789304\n",
      "Epoch 469 train loss: 0.7719710415005478 test loss: 1.302943926516958\n",
      "Epoch 470 train loss: 0.77542612663872 test loss: 1.2490131622844882\n",
      "Epoch 471 train loss: 0.7699959713511203 test loss: 1.2581558015599716\n",
      "Epoch 472 train loss: 0.7708451937759933 test loss: 1.2581978256141486\n",
      "Epoch 473 train loss: 0.7755477653314025 test loss: 1.265777163176124\n",
      "Epoch 474 train loss: 0.7735171450664043 test loss: 1.2411504014720633\n",
      "Epoch 475 train loss: 0.7693089819708111 test loss: 1.257233742045475\n",
      "Epoch 476 train loss: 0.7731127341064612 test loss: 1.2566429478786818\n",
      "Epoch 477 train loss: 0.7664824052426021 test loss: 1.271869303831405\n",
      "Epoch 478 train loss: 0.7693613802989239 test loss: 1.2556133305479065\n",
      "Epoch 479 train loss: 0.770816853829047 test loss: 1.2560505402964313\n",
      "Epoch 480 train loss: 0.7674215710135946 test loss: 1.2647582528162307\n",
      "Epoch 481 train loss: 0.7705154815570998 test loss: 1.2825326616377422\n",
      "Epoch 482 train loss: 0.7661605392647449 test loss: 1.263547832330506\n",
      "Epoch 483 train loss: 0.7679251054171868 test loss: 1.2897835746238822\n",
      "Epoch 484 train loss: 0.7691065023367067 test loss: 1.2589638480642167\n",
      "Epoch 485 train loss: 0.7669025580196484 test loss: 1.2733851754699241\n",
      "Epoch 486 train loss: 0.7727207342983171 test loss: 1.2502278795265434\n",
      "Epoch 487 train loss: 0.7628468260323691 test loss: 1.250597714692593\n",
      "Epoch 488 train loss: 0.766971928699266 test loss: 1.2868815875904156\n",
      "Epoch 489 train loss: 0.767989890838135 test loss: 1.2546948545458587\n",
      "Epoch 490 train loss: 0.7655963023784984 test loss: 1.2652789493902128\n",
      "Epoch 491 train loss: 0.7651981959432171 test loss: 1.2637644077136265\n",
      "Epoch 492 train loss: 0.7608128731043454 test loss: 1.2620729989770703\n",
      "Epoch 493 train loss: 0.7627069918960451 test loss: 1.2700013860314283\n",
      "Epoch 494 train loss: 0.7629036379478359 test loss: 1.2641920614422226\n",
      "Epoch 495 train loss: 0.7620086374685937 test loss: 1.2605020675582592\n",
      "Epoch 496 train loss: 0.781676056972488 test loss: 1.2578076070534676\n",
      "Epoch 497 train loss: 0.7574823091197356 test loss: 1.2600582879417646\n",
      "Epoch 498 train loss: 0.7660602872115759 test loss: 1.2593055567599678\n",
      "Epoch 499 train loss: 0.7580336021810915 test loss: 1.261191774744231\n",
      "Epoch 500 train loss: 0.7615359115560246 test loss: 1.285032450881814\n",
      "Epoch 501 train loss: 0.7597834432709164 test loss: 1.2939717455938742\n",
      "Epoch 502 train loss: 0.7605683232947611 test loss: 1.2637029473875976\n",
      "Epoch 503 train loss: 0.7623964724984622 test loss: 1.3119814688650338\n",
      "Epoch 504 train loss: 0.7549111683770664 test loss: 1.296511641761559\n",
      "Epoch 505 train loss: 0.762677040656531 test loss: 1.300207950311954\n",
      "Epoch 506 train loss: 0.7599179736930797 test loss: 1.2827739255156896\n",
      "Epoch 507 train loss: 0.7585250446803548 test loss: 1.2785131229833397\n",
      "Epoch 508 train loss: 0.7588768686861403 test loss: 1.2594647589066208\n",
      "Epoch 509 train loss: 0.7642027271311786 test loss: 1.2606762703258854\n",
      "Epoch 510 train loss: 0.7578600954488905 test loss: 1.2713810166047166\n",
      "Epoch 511 train loss: 0.7594664528881288 test loss: 1.2933529459215372\n",
      "Epoch 512 train loss: 0.7651892745097053 test loss: 1.2675359944341718\n",
      "Epoch 513 train loss: 0.7558280198543241 test loss: 1.2894368984030267\n",
      "Epoch 514 train loss: 0.7568890103199747 test loss: 1.2661130791540196\n",
      "Epoch 515 train loss: 0.7529701658380111 test loss: 1.30154920816936\n",
      "Epoch 516 train loss: 0.7558091593821944 test loss: 1.2600154542791884\n",
      "Epoch 517 train loss: 0.7541007524930924 test loss: 1.2569531903616982\n",
      "Epoch 518 train loss: 0.7550342337954421 test loss: 1.3089324664589896\n",
      "Epoch 519 train loss: 0.7559118569633378 test loss: 1.2756494515688892\n",
      "Epoch 520 train loss: 0.7585534369953035 test loss: 1.2670367424776716\n",
      "Epoch 521 train loss: 0.7640157078243113 test loss: 1.2662487897201387\n",
      "Epoch 522 train loss: 0.7589095036622874 test loss: 1.2857580844207392\n",
      "Epoch 523 train loss: 0.7644754519231908 test loss: 1.281818313879143\n",
      "Epoch 524 train loss: 0.7586578997924207 test loss: 1.2693299365612136\n",
      "Epoch 525 train loss: 0.7554150554040128 test loss: 1.2622436751294281\n",
      "Epoch 526 train loss: 0.7472971058831264 test loss: 1.296826123632172\n",
      "Epoch 527 train loss: 0.7583775915034843 test loss: 1.275466999183462\n",
      "Epoch 528 train loss: 0.7523200733056312 test loss: 1.2869333743589244\n",
      "Epoch 529 train loss: 0.7498708466059016 test loss: 1.2877080440810607\n",
      "Epoch 530 train loss: 0.7492584038331043 test loss: 1.275476466998643\n",
      "Epoch 531 train loss: 0.7487113844787325 test loss: 1.3058450782660498\n",
      "Epoch 532 train loss: 0.7524866757764433 test loss: 1.288595462002109\n",
      "Epoch 533 train loss: 0.7499649386578104 test loss: 1.2752525997966364\n",
      "Epoch 534 train loss: 0.7556786833407048 test loss: 1.284271502670561\n",
      "Epoch 535 train loss: 0.7556692235177701 test loss: 1.2772887594940134\n",
      "Epoch 536 train loss: 0.7531022556606645 test loss: 1.2965727775480014\n",
      "Epoch 537 train loss: 0.7512821837741255 test loss: 1.2965901414991905\n",
      "Epoch 538 train loss: 0.7509217226125583 test loss: 1.277863466493482\n",
      "Epoch 539 train loss: 0.7522325353844537 test loss: 1.2969032931657938\n",
      "Epoch 540 train loss: 0.7476958202236861 test loss: 1.286336606912804\n",
      "Epoch 541 train loss: 0.7462855430307785 test loss: 1.2821279404397035\n",
      "Epoch 542 train loss: 0.7549722152449584 test loss: 1.2973344847562736\n",
      "Epoch 543 train loss: 0.7485293818309395 test loss: 1.27845039104762\n",
      "Epoch 544 train loss: 0.7505215681801907 test loss: 1.2803202019956612\n",
      "Epoch 545 train loss: 0.747084070508898 test loss: 1.3118724964777586\n",
      "Epoch 546 train loss: 0.7615499151719379 test loss: 1.3054909433333803\n",
      "Epoch 547 train loss: 0.7465118964944961 test loss: 1.3037742187064008\n",
      "Epoch 548 train loss: 0.7395364185860862 test loss: 1.3028140829933543\n",
      "Epoch 549 train loss: 0.7502831226113916 test loss: 1.2778570092284574\n",
      "Epoch 550 train loss: 0.748654682134633 test loss: 1.2733720891234217\n",
      "Epoch 551 train loss: 0.7483454057379968 test loss: 1.313193935823515\n",
      "Epoch 552 train loss: 0.7482097176420596 test loss: 1.2765874524223342\n",
      "Epoch 553 train loss: 0.7521138772590142 test loss: 1.301760355932057\n",
      "Epoch 554 train loss: 0.7492318250813391 test loss: 1.2898225891553303\n",
      "Epoch 555 train loss: 0.7479191569305631 test loss: 1.3074637084817704\n",
      "Epoch 556 train loss: 0.7522516389662 test loss: 1.2709196163081113\n",
      "Epoch 557 train loss: 0.7447961393338073 test loss: 1.2599025861996584\n",
      "Epoch 558 train loss: 0.7463249182284393 test loss: 1.272954006974907\n",
      "Epoch 559 train loss: 0.7503482363946793 test loss: 1.2980310755818736\n",
      "Epoch 560 train loss: 0.7449502627257799 test loss: 1.2892233439427174\n",
      "Epoch 561 train loss: 0.7425794832489552 test loss: 1.3140364841965433\n",
      "Epoch 562 train loss: 0.7435341095399598 test loss: 1.2943718131705186\n",
      "Epoch 563 train loss: 0.7435803593671906 test loss: 1.2917604441111608\n",
      "Epoch 564 train loss: 0.7395428538672009 test loss: 1.2801079455192272\n",
      "Epoch 565 train loss: 0.7457748255902201 test loss: 1.3008167248096172\n",
      "Epoch 566 train loss: 0.7450077231037613 test loss: 1.3083743452445782\n",
      "Epoch 567 train loss: 0.7441470011297487 test loss: 1.3085062782246748\n",
      "Epoch 568 train loss: 0.7490774181494082 test loss: 1.2984485150369576\n",
      "Epoch 569 train loss: 0.743630252848905 test loss: 1.3180506879954708\n",
      "Epoch 570 train loss: 0.7456410349890822 test loss: 1.2804168093051982\n",
      "Epoch 571 train loss: 0.7380469946731204 test loss: 1.3075014245410412\n",
      "Epoch 572 train loss: 0.7437356171663405 test loss: 1.2995235303699935\n",
      "Epoch 573 train loss: 0.7408218839814852 test loss: 1.3019930007443061\n",
      "Epoch 574 train loss: 0.7443608855638054 test loss: 1.2909238441778066\n",
      "Epoch 575 train loss: 0.731840373258408 test loss: 1.3037966593280368\n",
      "Epoch 576 train loss: 0.7385234654190121 test loss: 1.3183380748327425\n",
      "Epoch 577 train loss: 0.7413468992467646 test loss: 1.2831593651704227\n",
      "Epoch 578 train loss: 0.7421459822829475 test loss: 1.3015070511531879\n",
      "Epoch 579 train loss: 0.7424722754772055 test loss: 1.3049901027968063\n",
      "Epoch 580 train loss: 0.7372368087620106 test loss: 1.2989613307892733\n",
      "Epoch 581 train loss: 0.7397535748546984 test loss: 1.2888432497623272\n",
      "Epoch 582 train loss: 0.743229535039158 test loss: 1.3208542386895994\n",
      "Epoch 583 train loss: 0.7516866608517979 test loss: 1.3120050140856971\n",
      "Epoch 584 train loss: 0.7486047579485988 test loss: 1.3071692326155375\n",
      "Epoch 585 train loss: 0.737638117604498 test loss: 1.2760105574318539\n",
      "Epoch 586 train loss: 0.741053792287404 test loss: 1.285098662299447\n",
      "Epoch 587 train loss: 0.7333906025295546 test loss: 1.3005403013019539\n",
      "Epoch 588 train loss: 0.7358606843257394 test loss: 1.2861618603479332\n",
      "Epoch 589 train loss: 0.7385688543631637 test loss: 1.2837083826579128\n",
      "Epoch 590 train loss: 0.7347693912167967 test loss: 1.2972433091210687\n",
      "Epoch 591 train loss: 0.736305130666978 test loss: 1.2922443462433106\n",
      "Epoch 592 train loss: 0.7361627242104353 test loss: 1.2957201396315905\n",
      "Epoch 593 train loss: 0.7413345253999383 test loss: 1.3233033279580886\n",
      "Epoch 594 train loss: 0.7357995737526798 test loss: 1.2789801355274009\n",
      "Epoch 595 train loss: 0.7368788168026176 test loss: 1.3086015924129333\n",
      "Epoch 596 train loss: 0.740412098140268 test loss: 1.3135992511437466\n",
      "Epoch 597 train loss: 0.7387397820100043 test loss: 1.3007550133438273\n",
      "Epoch 598 train loss: 0.736025642585526 test loss: 1.295812550572344\n",
      "Epoch 599 train loss: 0.7357269784730515 test loss: 1.3175330436416803\n",
      "Epoch 600 train loss: 0.7359699908739213 test loss: 1.304459579269111\n",
      "Epoch 601 train loss: 0.732164383775073 test loss: 1.3089723775710702\n",
      "Epoch 602 train loss: 0.7292533565375938 test loss: 1.3033998370495086\n",
      "Epoch 603 train loss: 0.7291044367034145 test loss: 1.3030772216625754\n",
      "Epoch 604 train loss: 0.7332798161663472 test loss: 1.3195021660920927\n",
      "Epoch 605 train loss: 0.7359281720777233 test loss: 1.3103252122987676\n",
      "Epoch 606 train loss: 0.7369316981367625 test loss: 1.292795465196245\n",
      "Epoch 607 train loss: 0.7328798950398699 test loss: 1.2978181433080365\n",
      "Epoch 608 train loss: 0.728135742336222 test loss: 1.3311070709953876\n",
      "Epoch 609 train loss: 0.7292105412778453 test loss: 1.2970264172513717\n",
      "Epoch 610 train loss: 0.7295853973147559 test loss: 1.318216306191022\n",
      "Epoch 611 train loss: 0.7308223911550862 test loss: 1.3019192716183055\n",
      "Epoch 612 train loss: 0.7336777328584515 test loss: 1.3173610891608858\n",
      "Epoch 613 train loss: 0.7314048841893933 test loss: 1.305958994161351\n",
      "Epoch 614 train loss: 0.7286317201647323 test loss: 1.3019639351211703\n",
      "Epoch 615 train loss: 0.7308335299604938 test loss: 1.3143705947353344\n",
      "Epoch 616 train loss: 0.7277593416161112 test loss: 1.3136394852900617\n",
      "Epoch 617 train loss: 0.7260104953956441 test loss: 1.3211855173479732\n",
      "Epoch 618 train loss: 0.733431684545653 test loss: 1.3405263130696812\n",
      "Epoch 619 train loss: 0.73068894153137 test loss: 1.3021732464411961\n",
      "Epoch 620 train loss: 0.7301355089820281 test loss: 1.3375763931872369\n",
      "Epoch 621 train loss: 0.7347174389044617 test loss: 1.3296678293546926\n",
      "Epoch 622 train loss: 0.7266270547834894 test loss: 1.3145607235970016\n",
      "Epoch 623 train loss: 0.7261593633886384 test loss: 1.3166202062343313\n",
      "Epoch 624 train loss: 0.7302109652850808 test loss: 1.3374324563501698\n",
      "Epoch 625 train loss: 0.7289036008438176 test loss: 1.3382658232464035\n",
      "Epoch 626 train loss: 0.7209916151502435 test loss: 1.3145044423730734\n",
      "Epoch 627 train loss: 0.7311752398193889 test loss: 1.2980881568111942\n",
      "Epoch 628 train loss: 0.7297461889795577 test loss: 1.349379309495297\n",
      "Epoch 629 train loss: 0.7316069613642446 test loss: 1.2976823716600618\n",
      "Epoch 630 train loss: 0.7282965552124632 test loss: 1.2859074873675955\n",
      "Epoch 631 train loss: 0.7269351474802197 test loss: 1.324746914680786\n",
      "Epoch 632 train loss: 0.7227821422225996 test loss: 1.3090598579260222\n",
      "Epoch 633 train loss: 0.7216802474820936 test loss: 1.3125773880375544\n",
      "Epoch 634 train loss: 0.7279643712456202 test loss: 1.3041448156286508\n",
      "Epoch 635 train loss: 0.7234139108168451 test loss: 1.3242695985832578\n",
      "Epoch 636 train loss: 0.7253837599128857 test loss: 1.3180752382972756\n",
      "Epoch 637 train loss: 0.7280011598355651 test loss: 1.3346610321816184\n",
      "Epoch 638 train loss: 0.7287216224112109 test loss: 1.3205088328919827\n",
      "Epoch 639 train loss: 0.7313877490191696 test loss: 1.3304307051236113\n",
      "Epoch 640 train loss: 0.7322574364570793 test loss: 1.3156700754254589\n",
      "Epoch 641 train loss: 0.7194943571974287 test loss: 1.3240593625665078\n",
      "Epoch 642 train loss: 0.7290256062229084 test loss: 1.3092039186230138\n",
      "Epoch 643 train loss: 0.7270267847841512 test loss: 1.3363820224420526\n",
      "Epoch 644 train loss: 0.7204126424531493 test loss: 1.3164301330431707\n",
      "Epoch 645 train loss: 0.7302191270674173 test loss: 1.3187263813307348\n",
      "Epoch 646 train loss: 0.7199473941385408 test loss: 1.3263635576913622\n",
      "Epoch 647 train loss: 0.7261413714825152 test loss: 1.3129085321131555\n",
      "Epoch 648 train loss: 0.7235706570010245 test loss: 1.329020790961162\n",
      "Epoch 649 train loss: 0.727700074670091 test loss: 1.3224174765388126\n",
      "Epoch 650 train loss: 0.7244813295070228 test loss: 1.3309858238839571\n",
      "Epoch 651 train loss: 0.7247861813554471 test loss: 1.3037149062686084\n",
      "Epoch 652 train loss: 0.7234453180613608 test loss: 1.3225234786408397\n",
      "Epoch 653 train loss: 0.7182478408634552 test loss: 1.3220459865436927\n",
      "Epoch 654 train loss: 0.7197459346152637 test loss: 1.3207840158222042\n",
      "Epoch 655 train loss: 0.7276417141408402 test loss: 1.3235548843195204\n",
      "Epoch 656 train loss: 0.7208200000641277 test loss: 1.313462544531949\n",
      "Epoch 657 train loss: 0.725316051150697 test loss: 1.3249765501606157\n",
      "Epoch 658 train loss: 0.7265283597719383 test loss: 1.309678980999942\n",
      "Epoch 659 train loss: 0.7208330772647539 test loss: 1.3378998018213166\n",
      "Epoch 660 train loss: 0.720023863826896 test loss: 1.3343855494838586\n",
      "Epoch 661 train loss: 0.7203330220295577 test loss: 1.3018440712279231\n",
      "Epoch 662 train loss: 0.7156473705171078 test loss: 1.3209313654622368\n",
      "Epoch 663 train loss: 0.7201295615810265 test loss: 1.3239748191532152\n",
      "Epoch 664 train loss: 0.7166610987751587 test loss: 1.3603659784756004\n",
      "Epoch 665 train loss: 0.7176525558033809 test loss: 1.305811836568389\n",
      "Epoch 666 train loss: 0.7166578310145184 test loss: 1.3170483769724894\n",
      "Epoch 667 train loss: 0.7115650340630596 test loss: 1.3083459634780354\n",
      "Epoch 668 train loss: 0.716966839167707 test loss: 1.3281174304854224\n",
      "Epoch 669 train loss: 0.7147710708725903 test loss: 1.314891795526839\n",
      "Epoch 670 train loss: 0.7181295336172432 test loss: 1.323985322629411\n",
      "Epoch 671 train loss: 0.726452948914446 test loss: 1.3505605260633813\n",
      "Epoch 672 train loss: 0.7114463519405357 test loss: 1.3249027464928993\n",
      "Epoch 673 train loss: 0.7197880494805297 test loss: 1.3496182130813472\n",
      "Epoch 674 train loss: 0.7192327563657857 test loss: 1.3267381135265452\n",
      "Epoch 675 train loss: 0.7121636372060959 test loss: 1.3260651753475794\n",
      "Epoch 676 train loss: 0.7237831303511053 test loss: 1.355615775442049\n",
      "Epoch 677 train loss: 0.7235586374491134 test loss: 1.3114079372489147\n",
      "Epoch 678 train loss: 0.7142741165626427 test loss: 1.3386101761722293\n",
      "Epoch 679 train loss: 0.7098908191700865 test loss: 1.3405547702098095\n",
      "Epoch 680 train loss: 0.7161594463864102 test loss: 1.3510581122321923\n",
      "Epoch 681 train loss: 0.7142036336113579 test loss: 1.3137948712719711\n",
      "Epoch 682 train loss: 0.7182225827031014 test loss: 1.335839713891174\n",
      "Epoch 683 train loss: 0.7123809561949515 test loss: 1.3048971659297632\n",
      "Epoch 684 train loss: 0.7128314342191713 test loss: 1.3409161196642447\n",
      "Epoch 685 train loss: 0.719889035324131 test loss: 1.3398053090079305\n",
      "Epoch 686 train loss: 0.7136859532838349 test loss: 1.3083930746502357\n",
      "Epoch 687 train loss: 0.714669799601427 test loss: 1.341969984418747\n",
      "Epoch 688 train loss: 0.7216492263799658 test loss: 1.3080254627712316\n",
      "Epoch 689 train loss: 0.7122211692100336 test loss: 1.3199783405589187\n",
      "Epoch 690 train loss: 0.7136160302297008 test loss: 1.3662370247107214\n",
      "Epoch 691 train loss: 0.7149766434050886 test loss: 1.3211258155218384\n",
      "Epoch 692 train loss: 0.706491568735478 test loss: 1.3322669983446114\n",
      "Epoch 693 train loss: 0.712189393116364 test loss: 1.3071289561360229\n",
      "Epoch 694 train loss: 0.7092009011330875 test loss: 1.312202580634989\n",
      "Epoch 695 train loss: 0.7169672341137975 test loss: 1.332733425131984\n",
      "Epoch 696 train loss: 0.7149069216716127 test loss: 1.3280817306409882\n",
      "Epoch 697 train loss: 0.708525437323725 test loss: 1.3326764910006523\n",
      "Epoch 698 train loss: 0.7136470230808879 test loss: 1.3203122844011226\n",
      "Epoch 699 train loss: 0.7195050349645606 test loss: 1.3425260436103341\n",
      "Epoch 700 train loss: 0.7075966705338529 test loss: 1.3241973919966261\n",
      "Epoch 701 train loss: 0.7136071969198492 test loss: 1.3206381966966971\n",
      "Epoch 702 train loss: 0.7091280037598545 test loss: 1.318998030375477\n",
      "Epoch 703 train loss: 0.7180308351353779 test loss: 1.3205264044719107\n",
      "Epoch 704 train loss: 0.719810440746524 test loss: 1.331909358533256\n",
      "Epoch 705 train loss: 0.7045406327301819 test loss: 1.3299305444593632\n",
      "Epoch 706 train loss: 0.7073183753015406 test loss: 1.3452441224897462\n",
      "Epoch 707 train loss: 0.7098728049008014 test loss: 1.3142552505273906\n",
      "Epoch 708 train loss: 0.7070819256999614 test loss: 1.3383961866975094\n",
      "Epoch 709 train loss: 0.7095329691748515 test loss: 1.336078196340247\n",
      "Epoch 710 train loss: 0.7119460985882939 test loss: 1.3470124795357765\n",
      "Epoch 711 train loss: 0.7062390468014421 test loss: 1.346863713774467\n",
      "Epoch 712 train loss: 0.7116015301255898 test loss: 1.352789055817739\n",
      "Epoch 713 train loss: 0.7061784704095424 test loss: 1.3346543927431584\n",
      "Epoch 714 train loss: 0.7063061898507419 test loss: 1.3526415907231506\n",
      "Epoch 715 train loss: 0.7070508579133352 test loss: 1.3449345490797195\n",
      "Epoch 716 train loss: 0.7206008679516054 test loss: 1.3504288346274662\n",
      "Epoch 717 train loss: 0.7159009416502105 test loss: 1.3121398821783565\n",
      "Epoch 718 train loss: 0.7020959414715162 test loss: 1.3089304759398557\n",
      "Epoch 719 train loss: 0.7058345551590692 test loss: 1.3163117887681004\n",
      "Epoch 720 train loss: 0.7074487811217548 test loss: 1.333046981701595\n",
      "Epoch 721 train loss: 0.7042285953360224 test loss: 1.3614205454528416\n",
      "Epoch 722 train loss: 0.705394133197839 test loss: 1.3506885672296867\n",
      "Epoch 723 train loss: 0.7035048450868836 test loss: 1.3287307891570996\n",
      "Epoch 724 train loss: 0.7063125800837287 test loss: 1.3462892561406599\n",
      "Epoch 725 train loss: 0.7085997022692212 test loss: 1.371235587663766\n",
      "Epoch 726 train loss: 0.7093703038828746 test loss: 1.323052247073\n",
      "Epoch 727 train loss: 0.7052049006198173 test loss: 1.331512270551186\n",
      "Epoch 728 train loss: 0.7064714482685587 test loss: 1.3195635536829502\n",
      "Epoch 729 train loss: 0.7055891772055399 test loss: 1.3434312595019875\n",
      "Epoch 730 train loss: 0.707375891815831 test loss: 1.3201982389469422\n",
      "Epoch 731 train loss: 0.7079799553110298 test loss: 1.3386913176873798\n",
      "Epoch 732 train loss: 0.7089161952537502 test loss: 1.3213367148257729\n",
      "Epoch 733 train loss: 0.7063285171887671 test loss: 1.3909380267976417\n",
      "Epoch 734 train loss: 0.7058700046347626 test loss: 1.3551928875661656\n",
      "Epoch 735 train loss: 0.6994887493415156 test loss: 1.3654721721978416\n",
      "Epoch 736 train loss: 0.7042421030018032 test loss: 1.3392577781453314\n",
      "Epoch 737 train loss: 0.7011058887938333 test loss: 1.3301396551272484\n",
      "Epoch 738 train loss: 0.7030713778975232 test loss: 1.3545876119118845\n",
      "Epoch 739 train loss: 0.7005197459191908 test loss: 1.3721728254468424\n",
      "Epoch 740 train loss: 0.6954784433383815 test loss: 1.3555434736144465\n",
      "Epoch 741 train loss: 0.7020092963198444 test loss: 1.3567399608955533\n",
      "Epoch 742 train loss: 0.7015570398626949 test loss: 1.3256909373558519\n",
      "Epoch 743 train loss: 0.6996173228500675 test loss: 1.3465250003785605\n",
      "Epoch 744 train loss: 0.7055460251226551 test loss: 1.3361495017812912\n",
      "Epoch 745 train loss: 0.6971699089935438 test loss: 1.3745064729062086\n",
      "Epoch 746 train loss: 0.6987020783621501 test loss: 1.3442141242210837\n",
      "Epoch 747 train loss: 0.7138837421738052 test loss: 1.3514429718652607\n",
      "Epoch 748 train loss: 0.7067974063658908 test loss: 1.3426271773649985\n",
      "Epoch 749 train loss: 0.7023985190080361 test loss: 1.3726465598754034\n",
      "Epoch 750 train loss: 0.7022662782610383 test loss: 1.3464434560715184\n",
      "Epoch 751 train loss: 0.7106280458097477 test loss: 1.3333532831926314\n",
      "Epoch 752 train loss: 0.7030950706497343 test loss: 1.3551873134409271\n",
      "Epoch 753 train loss: 0.7039718514835781 test loss: 1.3123199560229983\n",
      "Epoch 754 train loss: 0.701373383292627 test loss: 1.3532929187004652\n",
      "Epoch 755 train loss: 0.7047743061741386 test loss: 1.3524115491991853\n",
      "Epoch 756 train loss: 0.7056654556409576 test loss: 1.3438120273944296\n",
      "Epoch 757 train loss: 0.7039466576524228 test loss: 1.3285921608327267\n",
      "Epoch 758 train loss: 0.703665072940038 test loss: 1.3537243772776832\n",
      "Epoch 759 train loss: 0.7014046122422951 test loss: 1.3362255200628157\n",
      "Epoch 760 train loss: 0.694029345572246 test loss: 1.350750882985645\n",
      "Epoch 761 train loss: 0.6962219067448602 test loss: 1.3293219630981326\n",
      "Epoch 762 train loss: 0.7018414535583009 test loss: 1.3523336600255478\n",
      "Epoch 763 train loss: 0.7001124010832035 test loss: 1.3334489468873658\n",
      "Epoch 764 train loss: 0.7007036687625792 test loss: 1.3493081737840709\n",
      "Epoch 765 train loss: 0.7054918854386444 test loss: 1.362768686940337\n",
      "Epoch 766 train loss: 0.6980978696061249 test loss: 1.3378335837011832\n",
      "Epoch 767 train loss: 0.7056788401352857 test loss: 1.3459337146311148\n",
      "Epoch 768 train loss: 0.7072410808151469 test loss: 1.3470095501240196\n",
      "Epoch 769 train loss: 0.6959281128669864 test loss: 1.3617337421831142\n",
      "Epoch 770 train loss: 0.7018763205490288 test loss: 1.3739328721058324\n",
      "Epoch 771 train loss: 0.7002536304163256 test loss: 1.3391283391425404\n",
      "Epoch 772 train loss: 0.6975838397673166 test loss: 1.355168383702316\n",
      "Epoch 773 train loss: 0.6939841205401228 test loss: 1.3460204545489711\n",
      "Epoch 774 train loss: 0.6952117837615172 test loss: 1.338497211204933\n",
      "Epoch 775 train loss: 0.7003287393887012 test loss: 1.3213300165276436\n",
      "Epoch 776 train loss: 0.6955659074343228 test loss: 1.3415110176344327\n",
      "Epoch 777 train loss: 0.702437275445491 test loss: 1.350506313118682\n",
      "Epoch 778 train loss: 0.703322079554042 test loss: 1.3637317567908847\n",
      "Epoch 779 train loss: 0.703623321261348 test loss: 1.3328439061620787\n",
      "Epoch 780 train loss: 0.6901827779554083 test loss: 1.3563385484457724\n",
      "Epoch 781 train loss: 0.6999490424659748 test loss: 1.359286574500965\n",
      "Epoch 782 train loss: 0.6924817113726774 test loss: 1.3631412625119617\n",
      "Epoch 783 train loss: 0.6958418678692656 test loss: 1.3342965812660477\n",
      "Epoch 784 train loss: 0.6931691224641491 test loss: 1.3642131505013033\n",
      "Epoch 785 train loss: 0.7049472456769663 test loss: 1.3458911081770375\n",
      "Epoch 786 train loss: 0.7014869724865653 test loss: 1.3376726581999754\n",
      "Epoch 787 train loss: 0.6925170790889605 test loss: 1.3362198095349735\n",
      "Epoch 788 train loss: 0.6989294251976664 test loss: 1.3472473599094879\n",
      "Epoch 789 train loss: 0.7018488223674327 test loss: 1.3568052038537037\n",
      "Epoch 790 train loss: 0.6976135654096558 test loss: 1.3471934219830226\n",
      "Epoch 791 train loss: 0.6948117899076935 test loss: 1.3723142513793607\n",
      "Epoch 792 train loss: 0.7006716495850661 test loss: 1.351145435302013\n",
      "Epoch 793 train loss: 0.6956826787567497 test loss: 1.343951744254508\n",
      "Epoch 794 train loss: 0.6938292866726744 test loss: 1.366475074859437\n",
      "Epoch 795 train loss: 0.6953581287792959 test loss: 1.3503646170945083\n",
      "Epoch 796 train loss: 0.6974896115481225 test loss: 1.3513646934174814\n",
      "Epoch 797 train loss: 0.6953961194529608 test loss: 1.3445563589397387\n",
      "Epoch 798 train loss: 0.6958617335202127 test loss: 1.3845155286200854\n",
      "Epoch 799 train loss: 0.6900664383280352 test loss: 1.3426351826791754\n",
      "Epoch 800 train loss: 0.6876583157018173 test loss: 1.354431040553862\n",
      "Epoch 801 train loss: 0.6920465661968671 test loss: 1.3449157263558813\n",
      "Epoch 802 train loss: 0.7009972265009654 test loss: 1.3566698017667325\n",
      "Epoch 803 train loss: 0.6996701124178388 test loss: 1.390340262024428\n",
      "Epoch 804 train loss: 0.6867580625484545 test loss: 1.3486720467889586\n",
      "Epoch 805 train loss: 0.6916580187499964 test loss: 1.3688433071883295\n",
      "Epoch 806 train loss: 0.6967026818778537 test loss: 1.3780768128928025\n",
      "Epoch 807 train loss: 0.6931230532476527 test loss: 1.3512385760600276\n",
      "Epoch 808 train loss: 0.694111901646411 test loss: 1.3522324696316206\n",
      "Epoch 809 train loss: 0.6892339188366924 test loss: 1.3402468340983376\n",
      "Epoch 810 train loss: 0.6922774819581853 test loss: 1.3583550914614098\n",
      "Epoch 811 train loss: 0.6896113889439626 test loss: 1.364159809589645\n",
      "Epoch 812 train loss: 0.6949348473627511 test loss: 1.3787889948396956\n",
      "Epoch 813 train loss: 0.6923361062439823 test loss: 1.3166552158008515\n",
      "Epoch 814 train loss: 0.6946803314165984 test loss: 1.3439623327450212\n",
      "Epoch 815 train loss: 0.6923993682030523 test loss: 1.3426612428771225\n",
      "Epoch 816 train loss: 0.6909142330440685 test loss: 1.3545654979377035\n",
      "Epoch 817 train loss: 0.6909887348265354 test loss: 1.3716393448975341\n",
      "Epoch 818 train loss: 0.6889485359976611 test loss: 1.3524631445764816\n",
      "Epoch 819 train loss: 0.6835152026535354 test loss: 1.3596927797051581\n",
      "Epoch 820 train loss: 0.6910289464091399 test loss: 1.3606806438237915\n",
      "Epoch 821 train loss: 0.6853382569782827 test loss: 1.3683449918225814\n",
      "Epoch 822 train loss: 0.6902120673231408 test loss: 1.3527731718269078\n",
      "Epoch 823 train loss: 0.703467180844266 test loss: 1.3368639618079952\n",
      "Epoch 824 train loss: 0.6944187439173103 test loss: 1.3659419785525362\n",
      "Epoch 825 train loss: 0.6877695227071549 test loss: 1.3650629373715748\n",
      "Epoch 826 train loss: 0.6822298204848722 test loss: 1.3479942149977728\n",
      "Epoch 827 train loss: 0.6916407210315385 test loss: 1.3854848647591158\n",
      "Epoch 828 train loss: 0.695005204362879 test loss: 1.3442544251918136\n",
      "Epoch 829 train loss: 0.6934000471373069 test loss: 1.3805575579278277\n",
      "Epoch 830 train loss: 0.6914206238857505 test loss: 1.3693962989168624\n",
      "Epoch 831 train loss: 0.697190480577382 test loss: 1.3547098262612862\n",
      "Epoch 832 train loss: 0.6855000241796742 test loss: 1.348078702441073\n",
      "Epoch 833 train loss: 0.6855018990915913 test loss: 1.359109041764261\n",
      "Epoch 834 train loss: 0.6878388302500639 test loss: 1.37316945178583\n",
      "Epoch 835 train loss: 0.6853467855287525 test loss: 1.3442299346694173\n",
      "Epoch 836 train loss: 0.6901046166366578 test loss: 1.36023244576206\n",
      "Epoch 837 train loss: 0.6897450558936582 test loss: 1.3549534156700624\n",
      "Epoch 838 train loss: 0.6860357276563721 test loss: 1.3537421204959337\n",
      "Epoch 839 train loss: 0.6856714211779188 test loss: 1.3904045386195145\n",
      "Epoch 840 train loss: 0.6935493538864338 test loss: 1.3651699311079808\n",
      "Epoch 841 train loss: 0.688719622829058 test loss: 1.3382639908588465\n",
      "Epoch 842 train loss: 0.6919389638020197 test loss: 1.3392202826342419\n",
      "Epoch 843 train loss: 0.6901597838815459 test loss: 1.348934228201781\n",
      "Epoch 844 train loss: 0.6889707729837595 test loss: 1.3719227945236008\n",
      "Epoch 845 train loss: 0.6868601487870001 test loss: 1.3729447056122033\n",
      "Epoch 846 train loss: 0.6955472860246514 test loss: 1.3611587207720715\n",
      "Epoch 847 train loss: 0.6846473442831359 test loss: 1.3610175255587826\n",
      "Epoch 848 train loss: 0.6828100311875005 test loss: 1.3622745522809772\n",
      "Epoch 849 train loss: 0.6861618316211345 test loss: 1.3744755368741197\n",
      "Epoch 850 train loss: 0.6947820058443217 test loss: 1.3665779394713646\n",
      "Epoch 851 train loss: 0.6905289737609857 test loss: 1.363572242068014\n",
      "Epoch 852 train loss: 0.6872185981665566 test loss: 1.3619551509821926\n",
      "Epoch 853 train loss: 0.6853035101646441 test loss: 1.3685318218035811\n",
      "Epoch 854 train loss: 0.6888180576838255 test loss: 1.3761875903096847\n",
      "Epoch 855 train loss: 0.6846778589657244 test loss: 1.3524479922211086\n",
      "Epoch 856 train loss: 0.6862914031189409 test loss: 1.3954393443119082\n",
      "Epoch 857 train loss: 0.677700924332211 test loss: 1.3473249523705357\n",
      "Epoch 858 train loss: 0.685430511973884 test loss: 1.3716358978291248\n",
      "Epoch 859 train loss: 0.6870229627126256 test loss: 1.3688505725347184\n",
      "Epoch 860 train loss: 0.6882950210366507 test loss: 1.386759123885061\n",
      "Epoch 861 train loss: 0.6845902381344882 test loss: 1.3878062693817363\n",
      "Epoch 862 train loss: 0.6843022284158957 test loss: 1.400274232958902\n",
      "Epoch 863 train loss: 0.6780176139088988 test loss: 1.3608697414799384\n",
      "Epoch 864 train loss: 0.6859923250680621 test loss: 1.370794934037311\n",
      "Epoch 865 train loss: 0.6785184214047527 test loss: 1.3739302550442678\n",
      "Epoch 866 train loss: 0.6858331211931729 test loss: 1.371273802483624\n",
      "Epoch 867 train loss: 0.6808172471989135 test loss: 1.3808003505621833\n",
      "Epoch 868 train loss: 0.6803101574487073 test loss: 1.3661521314152572\n",
      "Epoch 869 train loss: 0.6808164978280742 test loss: 1.3550777548217652\n",
      "Epoch 870 train loss: 0.6796409718770143 test loss: 1.367710114709832\n",
      "Epoch 871 train loss: 0.6815805853165738 test loss: 1.3847761117521569\n",
      "Epoch 872 train loss: 0.6840522917178702 test loss: 1.3848080579304858\n",
      "Epoch 873 train loss: 0.6916861674041894 test loss: 1.363484753734017\n",
      "Epoch 874 train loss: 0.689243281753623 test loss: 1.3531948967660397\n",
      "Epoch 875 train loss: 0.6879933183157991 test loss: 1.3652056751948332\n",
      "Epoch 876 train loss: 0.6811686239412059 test loss: 1.3604299237682675\n",
      "Epoch 877 train loss: 0.6773574208598749 test loss: 1.3574190693557593\n",
      "Epoch 878 train loss: 0.6930219759685452 test loss: 1.3453896054148649\n",
      "Epoch 879 train loss: 0.6815403442409849 test loss: 1.3639672061762118\n",
      "Epoch 880 train loss: 0.6876491941987847 test loss: 1.3464488032446098\n",
      "Epoch 881 train loss: 0.6879819369925747 test loss: 1.3705823464539262\n",
      "Epoch 882 train loss: 0.6786800881092803 test loss: 1.3606202466541222\n",
      "Epoch 883 train loss: 0.6813349693776021 test loss: 1.3698912301432717\n",
      "Epoch 884 train loss: 0.6828609295698201 test loss: 1.3609590698922716\n",
      "Epoch 885 train loss: 0.6850491193559464 test loss: 1.409051612977168\n",
      "Epoch 886 train loss: 0.6821705155199065 test loss: 1.366170986705391\n",
      "Epoch 887 train loss: 0.685927481437329 test loss: 1.3670185270203261\n",
      "Epoch 888 train loss: 0.6808516487494538 test loss: 1.3723005254674063\n",
      "Epoch 889 train loss: 0.68036477534434 test loss: 1.4040353408130786\n",
      "Epoch 890 train loss: 0.6798368446869691 test loss: 1.3892747658526883\n",
      "Epoch 891 train loss: 0.6770888098833979 test loss: 1.3770481507323713\n",
      "Epoch 892 train loss: 0.6804811254353023 test loss: 1.401724421010132\n",
      "Epoch 893 train loss: 0.6850832928211809 test loss: 1.3507093773713552\n",
      "Epoch 894 train loss: 0.6835444744091469 test loss: 1.3509685677824521\n",
      "Epoch 895 train loss: 0.6811059363076615 test loss: 1.3683513168057402\n",
      "Epoch 896 train loss: 0.6837895308616049 test loss: 1.3576508480983895\n",
      "Epoch 897 train loss: 0.6752676450128718 test loss: 1.38032313862167\n",
      "Epoch 898 train loss: 0.6764595342155204 test loss: 1.3796880306698502\n",
      "Epoch 899 train loss: 0.6830997955716915 test loss: 1.347634144164146\n",
      "Epoch 900 train loss: 0.6815131396232897 test loss: 1.3768662187346172\n",
      "Epoch 901 train loss: 0.682491574329131 test loss: 1.3576871663856311\n",
      "Epoch 902 train loss: 0.6817195592556067 test loss: 1.3662589712556774\n",
      "Epoch 903 train loss: 0.6811555039631358 test loss: 1.3567531307126142\n",
      "Epoch 904 train loss: 0.6775161697121819 test loss: 1.3921654248794424\n",
      "Epoch 905 train loss: 0.6867247279807355 test loss: 1.360894218648082\n",
      "Epoch 906 train loss: 0.6752831193354294 test loss: 1.3960326620188774\n",
      "Epoch 907 train loss: 0.683972564139504 test loss: 1.368505642002335\n",
      "Epoch 908 train loss: 0.6756183438388124 test loss: 1.3784930011803131\n",
      "Epoch 909 train loss: 0.6740779443226583 test loss: 1.3777278250074618\n",
      "Epoch 910 train loss: 0.681633663468974 test loss: 1.3681537800294687\n",
      "Epoch 911 train loss: 0.6725344380103538 test loss: 1.388325693571808\n",
      "Epoch 912 train loss: 0.6821052426526617 test loss: 1.388290624938268\n",
      "Epoch 913 train loss: 0.6823377461610858 test loss: 1.387100191253257\n",
      "Epoch 914 train loss: 0.6859659963286662 test loss: 1.385602477554427\n",
      "Epoch 915 train loss: 0.6779683875624395 test loss: 1.3671212325864257\n",
      "Epoch 916 train loss: 0.670313778766119 test loss: 1.3977469551840391\n",
      "Epoch 917 train loss: 0.67922123822325 test loss: 1.3840747665589348\n",
      "Epoch 918 train loss: 0.6865526735939267 test loss: 1.3636525382723519\n",
      "Epoch 919 train loss: 0.6761839190087121 test loss: 1.3750089660808111\n",
      "Epoch 920 train loss: 0.6735045411734868 test loss: 1.369912583492856\n",
      "Epoch 921 train loss: 0.6756813197193553 test loss: 1.3616503768872907\n",
      "Epoch 922 train loss: 0.6809574618739792 test loss: 1.380492614416474\n",
      "Epoch 923 train loss: 0.6739288555281336 test loss: 1.3822978649857465\n",
      "Epoch 924 train loss: 0.676885932925132 test loss: 1.3515154925110022\n",
      "Epoch 925 train loss: 0.673806978894676 test loss: 1.3958674865180032\n",
      "Epoch 926 train loss: 0.6737247253951165 test loss: 1.3683067628676664\n",
      "Epoch 927 train loss: 0.6737544476377733 test loss: 1.3763093593199232\n",
      "Epoch 928 train loss: 0.6771810284388773 test loss: 1.3750028960992509\n",
      "Epoch 929 train loss: 0.680062289747034 test loss: 1.382507248537095\n",
      "Epoch 930 train loss: 0.6758478821584195 test loss: 1.358086034742102\n",
      "Epoch 931 train loss: 0.6730637566191794 test loss: 1.37530035789303\n",
      "Epoch 932 train loss: 0.6700056865238783 test loss: 1.3943637804814721\n",
      "Epoch 933 train loss: 0.6756448019327971 test loss: 1.3618470073302742\n",
      "Epoch 934 train loss: 0.6790465977538692 test loss: 1.389109200279902\n",
      "Epoch 935 train loss: 0.6784973541111636 test loss: 1.3825541360915659\n",
      "Epoch 936 train loss: 0.6705883997500952 test loss: 1.3736021264895104\n",
      "Epoch 937 train loss: 0.6711497591278448 test loss: 1.3646160393875968\n",
      "Epoch 938 train loss: 0.6730551682127633 test loss: 1.3733789745963638\n",
      "Epoch 939 train loss: 0.6723665355227227 test loss: 1.3859003260681253\n",
      "Epoch 940 train loss: 0.669836390484018 test loss: 1.3792733364266603\n",
      "Epoch 941 train loss: 0.6710126561899047 test loss: 1.3872888302832422\n",
      "Epoch 942 train loss: 0.6729049812825643 test loss: 1.3720872450849075\n",
      "Epoch 943 train loss: 0.6764518908582998 test loss: 1.3650022983814656\n",
      "Epoch 944 train loss: 0.6758326484896477 test loss: 1.3747044238331783\n",
      "Epoch 945 train loss: 0.6675987471621748 test loss: 1.3879227460113568\n",
      "Epoch 946 train loss: 0.6786413194420753 test loss: 1.387087701965048\n",
      "Epoch 947 train loss: 0.6699033309073127 test loss: 1.3840622658345285\n",
      "Epoch 948 train loss: 0.6706579540257241 test loss: 1.354322227314219\n",
      "Epoch 949 train loss: 0.6764765163980048 test loss: 1.4076242082452866\n",
      "Epoch 950 train loss: 0.6788154966149395 test loss: 1.36942270188675\n",
      "Epoch 951 train loss: 0.6722288162878849 test loss: 1.3770807720670906\n",
      "Epoch 952 train loss: 0.6711034420985441 test loss: 1.3902714321464897\n",
      "Epoch 953 train loss: 0.6664718466911197 test loss: 1.3712372259395413\n",
      "Epoch 954 train loss: 0.6746826213374176 test loss: 1.374269111485705\n",
      "Epoch 955 train loss: 0.6711256621753625 test loss: 1.3854425294810628\n",
      "Epoch 956 train loss: 0.6693428812825473 test loss: 1.3666572824589345\n",
      "Epoch 957 train loss: 0.668792991337744 test loss: 1.3941183305046774\n",
      "Epoch 958 train loss: 0.6694106902402714 test loss: 1.3744422871669388\n",
      "Epoch 959 train loss: 0.6755147339698397 test loss: 1.3807897167614738\n",
      "Epoch 960 train loss: 0.6712430699219288 test loss: 1.3722234996898024\n",
      "Epoch 961 train loss: 0.6714685527284849 test loss: 1.3803017926249028\n",
      "Epoch 962 train loss: 0.6703880960230274 test loss: 1.3764209751055074\n",
      "Epoch 963 train loss: 0.6752957191019433 test loss: 1.3707391572855798\n",
      "Epoch 964 train loss: 0.6654794639703646 test loss: 1.4133651926239854\n",
      "Epoch 965 train loss: 0.6680485920849448 test loss: 1.3738838729318803\n",
      "Epoch 966 train loss: 0.6680654692407484 test loss: 1.3698203491442187\n",
      "Epoch 967 train loss: 0.6677205294576029 test loss: 1.382710508549432\n",
      "Epoch 968 train loss: 0.6750302593026989 test loss: 1.3587959975311463\n",
      "Epoch 969 train loss: 0.6642697594450344 test loss: 1.3713599987223315\n",
      "Epoch 970 train loss: 0.6752342590011916 test loss: 1.4082878396726455\n",
      "Epoch 971 train loss: 0.6659254595239973 test loss: 1.3886931204657404\n",
      "Epoch 972 train loss: 0.6679082444578407 test loss: 1.3777560116431262\n",
      "Epoch 973 train loss: 0.6666582541427445 test loss: 1.3888839845325522\n",
      "Epoch 974 train loss: 0.6713294141152002 test loss: 1.3892224003181615\n",
      "Epoch 975 train loss: 0.6671626003058407 test loss: 1.3808952639752903\n",
      "Epoch 976 train loss: 0.663273600537346 test loss: 1.3907048915739826\n",
      "Epoch 977 train loss: 0.6649674946639866 test loss: 1.3977247393261314\n",
      "Epoch 978 train loss: 0.668741499221894 test loss: 1.3633459299970552\n",
      "Epoch 979 train loss: 0.6711958443477862 test loss: 1.4096971943872483\n",
      "Epoch 980 train loss: 0.6730720486577022 test loss: 1.3561398586683922\n",
      "Epoch 981 train loss: 0.6715290610167582 test loss: 1.3882674616677715\n",
      "Epoch 982 train loss: 0.669463550607406 test loss: 1.3825018065825856\n",
      "Epoch 983 train loss: 0.6694017955472663 test loss: 1.3586196499644738\n",
      "Epoch 984 train loss: 0.6651834690718317 test loss: 1.3919903078180564\n",
      "Epoch 985 train loss: 0.6660689912194586 test loss: 1.4243620784023188\n",
      "Epoch 986 train loss: 0.6672159666447707 test loss: 1.3856651982348511\n",
      "Epoch 987 train loss: 0.6642373878974805 test loss: 1.3731156703022096\n",
      "Epoch 988 train loss: 0.67035662759428 test loss: 1.4113643615281244\n",
      "Epoch 989 train loss: 0.6727165734060059 test loss: 1.3790369167068113\n",
      "Epoch 990 train loss: 0.6708271773408553 test loss: 1.3925540569527075\n",
      "Epoch 991 train loss: 0.668588699103327 test loss: 1.3922567250026057\n",
      "Epoch 992 train loss: 0.6655551901236403 test loss: 1.3845215262438253\n",
      "Epoch 993 train loss: 0.6662769707529651 test loss: 1.3768911889448368\n",
      "Epoch 994 train loss: 0.6703182458418444 test loss: 1.3771640042551274\n",
      "Epoch 995 train loss: 0.6690889440427541 test loss: 1.375818131963762\n",
      "Epoch 996 train loss: 0.6728991913958953 test loss: 1.4073397751801175\n",
      "Epoch 997 train loss: 0.666245343627563 test loss: 1.4043183209226493\n",
      "Epoch 998 train loss: 0.6721059746820518 test loss: 1.4040987870587796\n",
      "Epoch 999 train loss: 0.6649454342927819 test loss: 1.4014763171132203\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Split train and test data\n",
    "causal_dataset = CausalDataset(node_data[:, :-1], node_data[:, [-1]])\n",
    "train_data, test_data = train_test_split(causal_dataset, test_size=0.2, random_state=1)\n",
    "\n",
    "# Define dataloader\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define NN model\n",
    "model = MLP(input_dim=9, hidden_dim=32, output_dim=1, n_layers=5)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1e-2)\n",
    "\n",
    "\n",
    "# Train model\n",
    "print('Training model...')\n",
    "full_train_loss = []\n",
    "full_test_loss = []\n",
    "for train_loop in range(1000):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, epochs=1, device=device, verbose=False)\n",
    "    test_loss = test(model, test_loader, criterion, device)\n",
    "    test_loss = np.mean(test_loss)\n",
    "\n",
    "    full_train_loss.append(train_loss[-1])\n",
    "    full_test_loss.append(test_loss)\n",
    "\n",
    "    print(f'Epoch {train_loop} train loss: {train_loss[-1]} test loss: {test_loss}')\n",
    "    # Plot the results\n",
    "plt.plot(full_train_loss, label='Train loss')\n",
    "plt.plot(full_test_loss, label='Test loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we reimplement the network using a GCN and input the causal graph as an adjacency matrix.\n",
    "# We then train the model using the same training and testing data as before.\n",
    "# Use torch_geometric to build GCN\n",
    "\n",
    "from torch_geometric.nn import GCNConv, DenseGCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as GeometricDataLoader\n",
    "from torch_geometric.nn import Sequential as GeometricSequential\n",
    "from torch_geometric.nn import models as GeometricModels\n",
    "\n",
    "# Make a function to convert the adjacency matrix to a edje_index\n",
    "# but ignore edges from the last variable to the other variables\n",
    "def adj_to_edge_index(adj):\n",
    "    edge_index = []\n",
    "    for i in range(adj.shape[0]):\n",
    "        for j in range(adj.shape[1]):\n",
    "            if adj[i, j] == 1 and i != adj.shape[0] - 1:\n",
    "                edge_index.append([i, j])\n",
    "    return torch.tensor(edge_index, dtype=torch.float).t().contiguous()\n",
    "\n",
    "# Define a Graph dataset\n",
    "# here we have the full x, except it is zero on the target variable\n",
    "class CausalGraphDataset(Dataset):\n",
    "    def __init__(self, X, target, adj_matrix):\n",
    "        self.X = torch.tensor(X, dtype=torch.double)\n",
    "        self.adj_matrix = torch.tensor(adj_matrix, dtype=torch.double)\n",
    "        self.edge_index = adj_to_edge_index(self.adj_matrix).long()\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx].clone()\n",
    "        x[self.target] = 0\n",
    "        x = x.unsqueeze(-1)\n",
    "        return Data(x=x, edge_index=self.edge_index), self.X[idx, self.target]\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, target):\n",
    "        super(GCN, self).__init__()\n",
    "        self.net = GeometricModels.GCN(in_channels=input_dim, hidden_channels=hidden_dim, out_channels=output_dim, num_layers=n_layers)\n",
    "        self.target = target\n",
    "\n",
    "    def forward(self, data):\n",
    "        batch_size = data.batch[-1].item() + 1\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        y_hat = self.net(x, edge_index)\n",
    "\n",
    "        reshaped_y_hat = y_hat.reshape(batch_size, -1)\n",
    "        return reshaped_y_hat[:, self.target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 0 train loss: 11.738382193539058 test loss: 11.879551183254156\n",
      "Epoch 1 train loss: 11.54572376977292 test loss: 11.713179472300993\n",
      "Epoch 2 train loss: 11.231620400655233 test loss: 11.16705036660549\n",
      "Epoch 3 train loss: 10.422016613429474 test loss: 10.165078368802229\n",
      "Epoch 4 train loss: 9.543641762687027 test loss: 9.474194992649544\n",
      "Epoch 5 train loss: 8.88291332215957 test loss: 8.77343418276358\n",
      "Epoch 6 train loss: 8.304362939118878 test loss: 8.230881993137382\n",
      "Epoch 7 train loss: 7.8026792690245195 test loss: 7.663290269500456\n",
      "Epoch 8 train loss: 7.554392447367724 test loss: 7.473244002081511\n",
      "Epoch 9 train loss: 7.220502850160584 test loss: 7.1925004119871625\n",
      "Epoch 10 train loss: 6.9949892878822295 test loss: 7.083596571821572\n",
      "Epoch 11 train loss: 6.854968455911263 test loss: 7.008241602060016\n",
      "Epoch 12 train loss: 6.789390945930925 test loss: 7.107433196572394\n",
      "Epoch 13 train loss: 6.674581198644881 test loss: 7.286951902445931\n",
      "Epoch 14 train loss: 6.593026598281282 test loss: 6.817022626747708\n",
      "Epoch 15 train loss: 6.425757784456159 test loss: 6.641014223212559\n",
      "Epoch 16 train loss: 6.358450489833717 test loss: 6.508899313293462\n",
      "Epoch 17 train loss: 6.36541785917346 test loss: 6.2898029071096\n",
      "Epoch 18 train loss: 6.264986008588636 test loss: 6.285542499421217\n",
      "Epoch 19 train loss: 6.212192089067378 test loss: 6.416135534731234\n",
      "Epoch 20 train loss: 6.1489680730267375 test loss: 6.244962055498401\n",
      "Epoch 21 train loss: 6.117756105544194 test loss: 6.2156354155260685\n",
      "Epoch 22 train loss: 6.05908552228383 test loss: 6.165099206935972\n",
      "Epoch 23 train loss: 6.064578175319208 test loss: 6.019878805846667\n",
      "Epoch 24 train loss: 5.9655252571773305 test loss: 6.020716845279356\n",
      "Epoch 25 train loss: 5.916407289735014 test loss: 6.317698066852188\n",
      "Epoch 26 train loss: 5.953694521751191 test loss: 5.970034057110606\n",
      "Epoch 27 train loss: 5.924595323926779 test loss: 6.230422324952308\n",
      "Epoch 28 train loss: 5.958646166807792 test loss: 5.97212107168219\n",
      "Epoch 29 train loss: 5.834675047281545 test loss: 5.826187567886594\n",
      "Epoch 30 train loss: 5.790131615385012 test loss: 5.984848457324469\n",
      "Epoch 31 train loss: 5.789196415465751 test loss: 5.888026736008676\n",
      "Epoch 32 train loss: 5.727314342271521 test loss: 5.758297059563987\n",
      "Epoch 33 train loss: 5.718099324093491 test loss: 5.7942001229386\n",
      "Epoch 34 train loss: 5.698719337311148 test loss: 5.791007963985208\n",
      "Epoch 35 train loss: 5.718905162168262 test loss: 5.741809649326118\n",
      "Epoch 36 train loss: 5.646525498982429 test loss: 5.73766571233377\n",
      "Epoch 37 train loss: 5.673608981398801 test loss: 5.761273924366332\n",
      "Epoch 38 train loss: 5.66558728362871 test loss: 6.2851755348880625\n",
      "Epoch 39 train loss: 5.636915789571705 test loss: 5.6384196438883825\n",
      "Epoch 40 train loss: 5.584400064287797 test loss: 5.8990712209051415\n",
      "Epoch 41 train loss: 5.606402445174877 test loss: 5.677323711267776\n",
      "Epoch 42 train loss: 5.513671104057148 test loss: 5.617083911931895\n",
      "Epoch 43 train loss: 5.5138782119592635 test loss: 5.592546149522504\n",
      "Epoch 44 train loss: 5.531059390546564 test loss: 5.574866984775196\n",
      "Epoch 45 train loss: 5.54853205518158 test loss: 5.577037403010991\n",
      "Epoch 46 train loss: 5.612849883492843 test loss: 5.516317988465165\n",
      "Epoch 47 train loss: 5.449419867116654 test loss: 5.685919375326547\n",
      "Epoch 48 train loss: 5.485784544407904 test loss: 5.894615687144646\n",
      "Epoch 49 train loss: 5.429217594849729 test loss: 5.5123939496988354\n",
      "Epoch 50 train loss: 5.455323891390562 test loss: 5.704912302703288\n",
      "Epoch 51 train loss: 5.404981339021125 test loss: 5.383843833986468\n",
      "Epoch 52 train loss: 5.423031626553825 test loss: 5.546715375797369\n",
      "Epoch 53 train loss: 5.4183054892650615 test loss: 5.569960749673831\n",
      "Epoch 54 train loss: 5.373223428332873 test loss: 5.407255633044771\n",
      "Epoch 55 train loss: 5.366127885530599 test loss: 5.458598924835415\n",
      "Epoch 56 train loss: 5.359637966431264 test loss: 5.374740241096902\n",
      "Epoch 57 train loss: 5.370443875772889 test loss: 5.428318313850919\n",
      "Epoch 58 train loss: 5.329692839427149 test loss: 5.487713339863809\n",
      "Epoch 59 train loss: 5.412055104288227 test loss: 5.4952250188140495\n",
      "Epoch 60 train loss: 5.317436005773755 test loss: 5.408307230885258\n",
      "Epoch 61 train loss: 5.302877924061578 test loss: 5.319362343325565\n",
      "Epoch 62 train loss: 5.2511348060567995 test loss: 5.308026630886331\n",
      "Epoch 63 train loss: 5.271641365151676 test loss: 5.304111041915453\n",
      "Epoch 64 train loss: 5.2252642763982555 test loss: 5.2519559333932655\n",
      "Epoch 65 train loss: 5.263369082964485 test loss: 5.401693353623715\n",
      "Epoch 66 train loss: 5.297592692788694 test loss: 5.550038167584804\n",
      "Epoch 67 train loss: 5.2608337416548725 test loss: 5.514176951877214\n",
      "Epoch 68 train loss: 5.2457458282185305 test loss: 5.239488011802768\n",
      "Epoch 69 train loss: 5.195804638240175 test loss: 5.337140884817179\n",
      "Epoch 70 train loss: 5.2422643235312885 test loss: 5.316406058923729\n",
      "Epoch 71 train loss: 5.201257225861426 test loss: 5.278177525915671\n",
      "Epoch 72 train loss: 5.207684866813274 test loss: 5.255990101900107\n",
      "Epoch 73 train loss: 5.237723128948277 test loss: 5.2178547233583\n",
      "Epoch 74 train loss: 5.196296921575263 test loss: 5.317413432954034\n",
      "Epoch 75 train loss: 5.2642279776911955 test loss: 5.333493608633237\n",
      "Epoch 76 train loss: 5.174709941066521 test loss: 5.2506244511285285\n",
      "Epoch 77 train loss: 5.142623966768836 test loss: 5.192915374115164\n",
      "Epoch 78 train loss: 5.100051593026864 test loss: 5.3384125014657915\n",
      "Epoch 79 train loss: 5.151691671675032 test loss: 5.359612039041904\n",
      "Epoch 80 train loss: 5.179915982008307 test loss: 5.559464279327216\n",
      "Epoch 81 train loss: 5.12635841687433 test loss: 5.223570180498724\n",
      "Epoch 82 train loss: 5.111937750065824 test loss: 5.166550170329535\n",
      "Epoch 83 train loss: 5.110314985705364 test loss: 5.192966133234947\n",
      "Epoch 84 train loss: 5.066134257360365 test loss: 5.1468261208403\n",
      "Epoch 85 train loss: 5.086315870036335 test loss: 5.176017787342781\n",
      "Epoch 86 train loss: 5.091835978491225 test loss: 5.2916432474823925\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "device = torch.device(\"cpu\")\n",
    "target = [5, 9]\n",
    "\n",
    "structural_dataset = CausalGraphDataset(node_data, target, true_causal_matrix)\n",
    "train_structural_data, test_structural_data = train_test_split(structural_dataset, test_size=0.2, random_state=1)\n",
    "\n",
    "train_structural_dataloader = GeometricDataLoader(train_structural_data, batch_size=batch_size, shuffle=True)\n",
    "test_structural_dataloader = GeometricDataLoader(test_structural_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define NN model\n",
    "gcn_model = GCN(input_dim=1, hidden_dim=32, output_dim=1, n_layers=5, target=target)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(gcn_model.parameters(), lr=0.1e-2)\n",
    "\n",
    "# Train model\n",
    "print('Training model...')\n",
    "full_train_loss = []\n",
    "full_test_loss = []\n",
    "for train_loop in range(1000):\n",
    "\n",
    "    if train_loop % 100 == 0:\n",
    "        # adjust learning rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1\n",
    "\n",
    "    train_loss = train(gcn_model, train_structural_dataloader, criterion, optimizer, epochs=1, device=device, verbose=False)\n",
    "    test_loss = test(gcn_model, test_structural_dataloader, criterion, device)\n",
    "    test_loss = np.mean(test_loss)\n",
    "\n",
    "    full_train_loss.append(train_loss[-1])\n",
    "    full_test_loss.append(test_loss)\n",
    "\n",
    "    print(f'Epoch {train_loop} train loss: {train_loss[-1]} test loss: {test_loss}')\n",
    "    # Plot the results\n",
    "plt.plot(full_train_loss, label='Train loss')\n",
    "plt.plot(full_test_loss, label='Test loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gcn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0039df2195e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save model and weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgcn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gcn_model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'gcn_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Save model and weights\n",
    "torch.save(gcn_model.state_dict(), 'gcn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "71f78216db5cb2f09350fda35ad9d9668a66a927586f044b9adeddeb17c1df4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
