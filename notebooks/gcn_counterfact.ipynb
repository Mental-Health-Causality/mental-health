{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import dataset utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pytorch_lightning.trainer import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import importlib\n",
    "if importlib.util.find_spec('ipywidgets') is not None:\n",
    "    from tqdm.auto import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sexo</th>\n",
       "      <th>Estado_civil</th>\n",
       "      <th>Status_empl</th>\n",
       "      <th>Licenca</th>\n",
       "      <th>Tipo_Resid</th>\n",
       "      <th>Residencia</th>\n",
       "      <th>Alcoolatra</th>\n",
       "      <th>Droga</th>\n",
       "      <th>Suic_familia</th>\n",
       "      <th>Dep_familia</th>\n",
       "      <th>...</th>\n",
       "      <th>Eixo I: Panico sem agorafobia</th>\n",
       "      <th>Eixo I: Fobia especifica</th>\n",
       "      <th>Eixo I: Fobia social</th>\n",
       "      <th>Eixo I: Obsessivo-compulsivo</th>\n",
       "      <th>Eixo I: Estresse pos-traumatico</th>\n",
       "      <th>Eixo I: Ansiedade generalizada</th>\n",
       "      <th>Eixo II: Personalidade paranoica</th>\n",
       "      <th>Eixo II: Transtorno de personalidade</th>\n",
       "      <th>TOC</th>\n",
       "      <th>idade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  sexo  Estado_civil  Status_empl  Licenca  Tipo_Resid  Residencia  \\\n",
       "0    M           3.0          NaN      0.0         3.0         1.0   \n",
       "1    F           1.0          3.0      0.0         4.0         3.0   \n",
       "2    F           1.0          2.0      0.0         1.0         2.0   \n",
       "3    F           1.0          3.0      0.0         1.0         3.0   \n",
       "4    F           4.0          2.0      0.0         1.0         NaN   \n",
       "\n",
       "   Alcoolatra  Droga  Suic_familia  Dep_familia  ...  \\\n",
       "0           0      0             0            1  ...   \n",
       "1           0      0             0            1  ...   \n",
       "2           0      0             1            1  ...   \n",
       "3           0      0             0            1  ...   \n",
       "4           1      0             0            1  ...   \n",
       "\n",
       "   Eixo I: Panico sem agorafobia  Eixo I: Fobia especifica  \\\n",
       "0                            0.0                       0.0   \n",
       "1                            NaN                       NaN   \n",
       "2                            0.0                       0.0   \n",
       "3                            NaN                       NaN   \n",
       "4                            0.0                       0.0   \n",
       "\n",
       "   Eixo I: Fobia social Eixo I: Obsessivo-compulsivo  \\\n",
       "0                   0.0                          0.0   \n",
       "1                   NaN                          NaN   \n",
       "2                   0.0                          0.0   \n",
       "3                   NaN                          NaN   \n",
       "4                   0.0                          0.0   \n",
       "\n",
       "   Eixo I: Estresse pos-traumatico  Eixo I: Ansiedade generalizada  \\\n",
       "0                              0.0                             0.0   \n",
       "1                              NaN                             NaN   \n",
       "2                              0.0                             0.0   \n",
       "3                              NaN                             NaN   \n",
       "4                              0.0                             0.0   \n",
       "\n",
       "   Eixo II: Personalidade paranoica  Eixo II: Transtorno de personalidade  \\\n",
       "0                               0.0                                   0.0   \n",
       "1                               NaN                                   NaN   \n",
       "2                               0.0                                   0.0   \n",
       "3                               NaN                                   NaN   \n",
       "4                               0.0                                   0.0   \n",
       "\n",
       "   TOC  idade  \n",
       "0  3.0   40.0  \n",
       "1  0.0   20.0  \n",
       "2  0.0   20.0  \n",
       "3  6.0   30.0  \n",
       "4  0.0   40.0  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('../data/final.csv', sep=';')\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all nans with -1\n",
    "dataframe = dataframe.fillna(-1)\n",
    "\n",
    "# drop Chave\n",
    "dataframe = dataframe.drop(['Chave'], axis=1)\n",
    "\n",
    "dataframe['sexo'].replace({'M': 0, 'F': 1}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3953, 68)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_suic = dataframe.copy()\n",
    "\n",
    "df_suic = df_suic.astype(float)\n",
    "\n",
    "df_suic.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn -5 to -1\n",
    "df_suic['Anos educacao formal'] = df_suic['Anos educacao formal'].replace(-5, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sexo: (-1.0, 1.0)\n",
      "Estado_civil: (-1.0, 6.0)\n",
      "Status_empl: (-1.0, 6.0)\n",
      "Licenca: (-1.0, 1.0)\n",
      "Tipo_Resid: (-1.0, 8.0)\n",
      "Residencia: (-1.0, 3.0)\n",
      "Alcoolatra: (0.0, 1.0)\n",
      "Droga: (0.0, 1.0)\n",
      "Suic_familia: (0.0, 1.0)\n",
      "Dep_familia: (0.0, 1.0)\n",
      "Bip_familia: (0.0, 1.0)\n",
      "Alc_familia: (0.0, 1.0)\n",
      "Drog_familia: (0.0, 1.0)\n",
      "coracao: (0.0, 4.0)\n",
      "vascular: (0.0, 3.0)\n",
      "hematopoetico: (0.0, 4.0)\n",
      "Olho_ore_nariz_garg_lar: (0.0, 4.0)\n",
      "GI_sup: (0.0, 4.0)\n",
      "Gi_inf: (0.0, 4.0)\n",
      "Renal: (0.0, 4.0)\n",
      "Genito_urinario: (0.0, 4.0)\n",
      "Musculoesqueletico: (0.0, 4.0)\n",
      "Neuro: (0.0, 4.0)\n",
      "psiquiatrica: (0.0, 4.0)\n",
      "Respiratorio: (0.0, 4.0)\n",
      "Figado: (0.0, 4.0)\n",
      "Endocrino_metabolico: (0.0, 4.0)\n",
      "Anos educacao formal: (-1.0, 27.0)\n",
      "Capaz de desfrutar das coisas: (-1.0, 5.0)\n",
      "Impacto de sua familia e amigos: (-1.0, 7.0)\n",
      "Numero de amigos vivendo com paciente: (-1.0, 89.0)\n",
      "Capaz de tomar decisÃµes importantes: (-1.0, 5.0)\n",
      "Numero de parentes vivendo com paciente: (-1.0, 11.0)\n",
      "Conjuge_companheiro vive com paciente: (-1.0, 1.0)\n",
      "Estudante: (-1.0, 1.0)\n",
      "Numero total de pessoas em casa: (-1.0, 90.0)\n",
      "Receber dinheiro do emprego: (-1.0, 1.0)\n",
      "Renda mensal: (-1.0, 50000.0)\n",
      "Recebe assistencia publica: (-1.0, 1.0)\n",
      "Insonia: (-1.0, 2.0)\n",
      "Insonia media: (-1.0, 2.0)\n",
      "Insonia tardia: (-1.0, 2.0)\n",
      "Deprimido: (-1.0, 4.0)\n",
      "Ansiedade: (-1.0, 4.0)\n",
      "Perda de insights: (-1.0, 2.0)\n",
      "Apetite: (-1.0, 2.0)\n",
      "Perda de peso: (-1.0, 2.0)\n",
      "Ansiedade somÃ¡tica: (-1.0, 4.0)\n",
      "Hipocondriase: (-1.0, 4.0)\n",
      "Sentimentos_culpa: (-1.0, 4.0)\n",
      "Suicidio: (-1.0, 4.0)\n",
      "Trabalho e interesses: (-1.0, 4.0)\n",
      "Energia: (-1.0, 2.0)\n",
      "Lentidao pensamento e fala: (-1.0, 3.0)\n",
      "AgitaÃ§Ã£o: (-1.0, 4.0)\n",
      "Libido: (-1.0, 2.0)\n",
      "PontuaÃ§Ã£o total: (-1.0, 43.0)\n",
      "Eixo I: Panico com agorafobia: (-1.0, 1.0)\n",
      "Eixo I: Panico sem agorafobia: (-1.0, 1.0)\n",
      "Eixo I: Fobia especifica: (-1.0, 1.0)\n",
      "Eixo I: Fobia social: (-1.0, 1.0)\n",
      "Eixo I: Obsessivo-compulsivo: (-1.0, 1.0)\n",
      "Eixo I: Estresse pos-traumatico: (-1.0, 1.0)\n",
      "Eixo I: Ansiedade generalizada: (-1.0, 1.0)\n",
      "Eixo II: Personalidade paranoica: (-1.0, 1.0)\n",
      "Eixo II: Transtorno de personalidade: (-1.0, 1.0)\n",
      "TOC: (-1.0, 8.0)\n",
      "idade: (-1.0, 70.0)\n"
     ]
    }
   ],
   "source": [
    "# find min and max values for each column\n",
    "min_max = {}\n",
    "for col in df_suic.columns:\n",
    "    min_max[col] = (df_suic[col].min(), df_suic[col].max())\n",
    "# preety print\n",
    "for k, v in min_max.items():\n",
    "    print(f'{k}: {v}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    " \n",
    "  def __init__(self, input_dataframe, split=\"train\", target=[\"Suicidio\", \"Ansiedade\"], ignore_columns=[], train_ratio=0.8):\n",
    "    \n",
    "    self.split = split\n",
    "    self.target = target\n",
    "    self.ignore_columns = ignore_columns\n",
    "\n",
    "    for coll in self.ignore_columns:\n",
    "       if coll in input_dataframe.columns:\n",
    "        input_dataframe = input_dataframe.drop(coll, axis=1)\n",
    "\n",
    "    # self.classification_dim = len(input_dataframe[self.target].unique())\n",
    "    self.data_dim = len(input_dataframe.columns) - len(target) - len(ignore_columns)\n",
    "    self.embbeding_dim = input_dataframe.max().max() + 1\n",
    "\n",
    "    y = input_dataframe[target].values\n",
    "    x = input_dataframe.drop(target, axis = 1).values\n",
    "\n",
    "    self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(x, y, test_size=1-train_ratio, random_state=42)\n",
    "\n",
    "  def __len__(self):\n",
    "    if self.split == \"train\":\n",
    "      return len(self.x_train)\n",
    "    elif self.split == \"test\":\n",
    "      return len(self.x_test)\n",
    "    else:\n",
    "      raise ValueError(\"Split must be train or test\")\n",
    "\n",
    "  def get_weights(self):\n",
    "    if self.split == \"train\":\n",
    "      y = self.y_train\n",
    "    elif self.split == \"test\":\n",
    "      y = self.y_test\n",
    "\n",
    "    weights = []\n",
    "    for i in range(len(self.target)):\n",
    "      weights_dict = pd.DataFrame(y[:,i]).value_counts(normalize=True).to_dict()\n",
    "      keys = sorted([k for k in weights_dict.keys()], key=lambda x: x[0])\n",
    "      weights.append(np.array([1/weights_dict[k] for k in keys]))\n",
    "    self.weights = np.hstack(weights).T\n",
    "\n",
    "    return self.weights\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    # target = torch.zeros(self.classification_dim)\n",
    "\n",
    "    if self.split == \"train\":\n",
    "      # target[self.y_train[idx]] = 1\n",
    "      target = torch.tensor(self.y_train[idx], dtype=torch.float)\n",
    "      return (torch.tensor(self.x_train[idx], dtype=torch.float), target)\n",
    "    elif self.split == \"test\":\n",
    "      # target[self.y_test[idx]] = 1\n",
    "      target = torch.tensor(self.y_test[idx], dtype=torch.float)\n",
    "      return (torch.tensor(self.x_test[idx], dtype=torch.float), target)\n",
    "    else:\n",
    "      raise ValueError(\"Split must be train or test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make embbeding layer with one different embbeding for each column and acount for the -1 values\n",
    "class MyEmbbeding(nn.Module):\n",
    "    def __init__(self, dataframe, fake=False):\n",
    "        super(MyEmbbeding, self).__init__()\n",
    "        self.embbedings = nn.ModuleList()\n",
    "        self.fake = fake\n",
    "        # create embbeding for each column in order\n",
    "        for col in dataframe.columns:\n",
    "            self.embbedings.append(nn.Embedding(int(dataframe[col].max()+10), 1))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.fake:\n",
    "            return x\n",
    "\n",
    "        embbedings = []\n",
    "        assert x.shape[1] == len(self.embbedings), f\"Input shape {x.shape} must be equal to number of embbedings {len(self.embbedings)}\"\n",
    "        for i, embbeding in enumerate(self.embbedings):\n",
    "            # print embbeding max label\n",
    "            embbedings.append(embbeding(x[:,i].long()+1))\n",
    "        return torch.cat(embbedings, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification model using the embbeding layer and output a one hot vector with the classification\n",
    "class NNModel(nn.Module):\n",
    "    def __init__(self, embbeding, hidden_dim=64, output_dim=1):\n",
    "        super(NNModel, self).__init__()\n",
    "        self.embbeding = embbeding\n",
    "        self.fc1 = nn.Linear(embbeding.embbedings[0].embedding_dim*len(embbeding.embbedings), hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    # regressive output\n",
    "    def forward(self, x):\n",
    "        x = self.embbeding(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MyDataset(df_suic, \"test\", target=[])\n",
    "train_dataset = MyDataset(df_suic, \"train\", target=[])\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 3162\n",
      "Test dataset size: 791\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "[0]Train loss 132416.15625, acc 0.4642329688285571\n",
      "[0]Test loss 99757.296875, acc 0.4873521419437341\n",
      "Epoch 1\n",
      "[1]Train loss 109626.0078125, acc 0.44282902554961384\n",
      "[1]Test loss 80379.0078125, acc 0.49748561381074163\n",
      "Epoch 2\n",
      "[2]Train loss 101628.734375, acc 0.4767985282691165\n",
      "[2]Test loss 79003.1171875, acc 0.47153852301790283\n",
      "Epoch 3\n",
      "[3]Train loss 98682.3515625, acc 0.4656866315873669\n",
      "[3]Test loss 75223.84375, acc 0.45780850383631716\n",
      "Epoch 4\n",
      "[4]Train loss 95153.4140625, acc 0.4804250079985374\n",
      "[4]Test loss 72132.2890625, acc 0.48916320332480817\n",
      "Epoch 5\n",
      "[5]Train loss 89695.8671875, acc 0.47847036541889487\n",
      "[5]Test loss 66738.8203125, acc 0.4754627557544757\n",
      "Epoch 6\n",
      "[6]Train loss 83050.140625, acc 0.4748624537227478\n",
      "[6]Test loss 61833.4609375, acc 0.5100999040920716\n",
      "Epoch 7\n",
      "[7]Train loss 76389.75, acc 0.48886167957859145\n",
      "[7]Test loss 56766.62890625, acc 0.44643142583120204\n",
      "Epoch 8\n",
      "[8]Train loss 71470.3359375, acc 0.48776687977055627\n",
      "[8]Test loss 54516.30078125, acc 0.507876438618926\n",
      "Epoch 9\n",
      "[9]Train loss 68293.40625, acc 0.4865117521367522\n",
      "[9]Test loss 53368.92578125, acc 0.4909766624040921\n",
      "Epoch 10\n",
      "[10]Train loss 66636.9921875, acc 0.4717094588418118\n",
      "[10]Test loss 51829.80078125, acc 0.46871243606138113\n",
      "Epoch 11\n",
      "[11]Train loss 64684.5546875, acc 0.49076775960967145\n",
      "[11]Test loss 51248.64453125, acc 0.4721651214833759\n",
      "Epoch 12\n",
      "[12]Train loss 63529.0234375, acc 0.48299775183966376\n",
      "[12]Test loss 50476.23046875, acc 0.5048345588235295\n",
      "Epoch 13\n",
      "[13]Train loss 62392.7734375, acc 0.49080632398647106\n",
      "[13]Test loss 50025.328125, acc 0.5012755754475704\n",
      "Epoch 14\n",
      "[14]Train loss 61236.5, acc 0.4876965354906531\n",
      "[14]Test loss 52799.984375, acc 0.4811213235294119\n",
      "Epoch 15\n",
      "[15]Train loss 60381.1015625, acc 0.4790152657799717\n",
      "[15]Test loss 49712.30078125, acc 0.4323721227621483\n",
      "Epoch 16\n",
      "[16]Train loss 59680.7265625, acc 0.48627036770419113\n",
      "[16]Test loss 51313.0546875, acc 0.4994005754475703\n",
      "Epoch 17\n",
      "[17]Train loss 58941.4375, acc 0.4838868749714337\n",
      "[17]Test loss 48318.35546875, acc 0.45434782608695656\n",
      "Epoch 18\n",
      "[18]Train loss 57929.5234375, acc 0.4822478889574478\n",
      "[18]Test loss 47948.98828125, acc 0.4630027173913044\n",
      "Epoch 19\n",
      "[19]Train loss 57111.0234375, acc 0.4835365818821701\n",
      "[19]Test loss 49893.140625, acc 0.5025279731457801\n",
      "Epoch 20\n",
      "[20]Train loss 56515.53515625, acc 0.4880411153389095\n",
      "[20]Test loss 47235.140625, acc 0.5125119884910486\n",
      "Epoch 21\n",
      "[21]Train loss 55795.0, acc 0.48871991978609636\n",
      "[21]Test loss 46828.8359375, acc 0.4635469948849104\n",
      "Epoch 22\n",
      "[22]Train loss 55422.36328125, acc 0.47037934491978617\n",
      "[22]Test loss 46970.89453125, acc 0.4779915281329922\n",
      "Epoch 23\n",
      "[23]Train loss 54554.18359375, acc 0.491830779514603\n",
      "[23]Test loss 46268.9453125, acc 0.5084183184143222\n",
      "Epoch 24\n",
      "[24]Train loss 53964.984375, acc 0.4810102581242287\n",
      "[24]Test loss 46379.390625, acc 0.43878356777493605\n",
      "Epoch 25\n",
      "[25]Train loss 54051.296875, acc 0.4611853118286942\n",
      "[25]Test loss 45454.37109375, acc 0.4593542199488491\n",
      "Epoch 26\n",
      "[26]Train loss 52870.51953125, acc 0.48980972050825\n",
      "[26]Test loss 50731.41015625, acc 0.5081377877237853\n",
      "Epoch 27\n",
      "[27]Train loss 52142.01171875, acc 0.49295914461355633\n",
      "[27]Test loss 48396.37109375, acc 0.5047722186700766\n",
      "Epoch 28\n",
      "[28]Train loss 51457.8046875, acc 0.4897493744001096\n",
      "[28]Test loss 45138.93359375, acc 0.5033871483375959\n",
      "Epoch 29\n",
      "[29]Train loss 51130.9140625, acc 0.49775576751679684\n",
      "[29]Test loss 44340.68359375, acc 0.4866224424552429\n",
      "Epoch 30\n",
      "[30]Train loss 50454.91015625, acc 0.4932062422871246\n",
      "[30]Test loss 44205.890625, acc 0.48759510869565215\n",
      "Epoch 31\n",
      "[31]Train loss 49923.4609375, acc 0.49230319313039894\n",
      "[31]Test loss 44420.5, acc 0.4845660166240409\n",
      "Epoch 32\n",
      "[32]Train loss 49318.84375, acc 0.4809552681795329\n",
      "[32]Test loss 43874.58984375, acc 0.5031026214833759\n",
      "Epoch 33\n",
      "[33]Train loss 48960.30859375, acc 0.481685848873349\n",
      "[33]Test loss 43550.69921875, acc 0.45270540281329924\n",
      "Epoch 34\n",
      "[34]Train loss 48434.390625, acc 0.4727303435943142\n",
      "[34]Test loss 43378.8046875, acc 0.49281329923273653\n",
      "Epoch 35\n",
      "[35]Train loss 47838.07421875, acc 0.4938732631747338\n",
      "[35]Test loss 43125.40625, acc 0.50923273657289\n",
      "Epoch 36\n",
      "[36]Train loss 47461.8125, acc 0.4899000611316788\n",
      "[36]Test loss 42793.578125, acc 0.5175767263427111\n",
      "Epoch 37\n",
      "[37]Train loss 46890.82421875, acc 0.4893858694410164\n",
      "[37]Test loss 42756.00390625, acc 0.4439042519181585\n",
      "Epoch 38\n",
      "[38]Train loss 46759.4140625, acc 0.47068785993418344\n",
      "[38]Test loss 43989.69921875, acc 0.495980658567775\n",
      "Epoch 39\n",
      "[39]Train loss 45981.5703125, acc 0.4917840023538554\n",
      "[39]Test loss 42262.6640625, acc 0.5164793797953964\n",
      "Epoch 40\n",
      "[40]Train loss 45813.76953125, acc 0.4836926247771836\n",
      "[40]Test loss 43343.9453125, acc 0.4980466751918158\n",
      "Epoch 41\n",
      "[41]Train loss 45200.18359375, acc 0.4952851478586773\n",
      "[41]Test loss 41771.55859375, acc 0.49661524936061374\n",
      "Epoch 42\n",
      "[42]Train loss 44827.1015625, acc 0.49941689233968656\n",
      "[42]Test loss 41706.53125, acc 0.5065321291560102\n",
      "Epoch 43\n",
      "[43]Train loss 44807.3984375, acc 0.4826267482517481\n",
      "[43]Test loss 41409.421875, acc 0.5172266624040921\n",
      "Epoch 44\n",
      "[44]Train loss 43948.515625, acc 0.48864457642031184\n",
      "[44]Test loss 42017.73046875, acc 0.44787324168797954\n",
      "Epoch 45\n",
      "[45]Train loss 43396.70703125, acc 0.45877825197678146\n",
      "[45]Test loss 44187.296875, acc 0.4590001598465474\n",
      "Epoch 46\n",
      "[46]Train loss 43028.08984375, acc 0.4793644876365465\n",
      "[46]Test loss 41439.0625, acc 0.4941592071611254\n",
      "Epoch 47\n",
      "[47]Train loss 42621.3828125, acc 0.48935194707253526\n",
      "[47]Test loss 43069.546875, acc 0.4985238171355499\n",
      "Epoch 48\n",
      "[48]Train loss 42252.890625, acc 0.49822746697746695\n",
      "[48]Test loss 41269.5234375, acc 0.49490569053708455\n",
      "Epoch 49\n",
      "[49]Train loss 41855.38671875, acc 0.5007516482700307\n",
      "[49]Test loss 40897.60546875, acc 0.503698049872123\n",
      "Epoch 50\n",
      "[50]Train loss 41723.75390625, acc 0.5002092474518944\n",
      "[50]Test loss 40582.09375, acc 0.5024896099744245\n",
      "Epoch 51\n",
      "[51]Train loss 41399.94140625, acc 0.4823853638191873\n",
      "[51]Test loss 40792.984375, acc 0.5051582480818414\n",
      "Epoch 52\n",
      "[52]Train loss 40889.26171875, acc 0.48615003256547373\n",
      "[52]Test loss 40245.0859375, acc 0.5112132352941176\n",
      "Epoch 53\n",
      "[53]Train loss 40584.89453125, acc 0.48979543740573156\n",
      "[53]Test loss 42761.8203125, acc 0.5088075447570332\n",
      "Epoch 54\n",
      "[54]Train loss 40254.65234375, acc 0.47494565279491735\n",
      "[54]Test loss 39970.87890625, acc 0.5003324808184144\n",
      "Epoch 55\n",
      "[55]Train loss 39845.05859375, acc 0.4929923528269117\n",
      "[55]Test loss 39830.7265625, acc 0.508360773657289\n",
      "Epoch 56\n",
      "[56]Train loss 39471.8203125, acc 0.48657816856346275\n",
      "[56]Test loss 39563.86328125, acc 0.503366368286445\n",
      "Epoch 57\n",
      "[57]Train loss 39021.98046875, acc 0.48300382215823395\n",
      "[57]Test loss 39728.84765625, acc 0.47626998081841426\n",
      "Epoch 58\n",
      "[58]Train loss 38598.21875, acc 0.4866231603363956\n",
      "[58]Test loss 39513.09375, acc 0.46126598465473156\n",
      "Epoch 59\n",
      "[59]Train loss 38198.05078125, acc 0.4873708807532337\n",
      "[59]Test loss 39861.77734375, acc 0.47044916879795395\n",
      "Epoch 60\n",
      "[60]Train loss 37971.06640625, acc 0.4755273321449791\n",
      "[60]Test loss 41204.8984375, acc 0.4875623401534527\n",
      "Epoch 61\n",
      "[61]Train loss 37624.578125, acc 0.4908256061748708\n",
      "[61]Test loss 39542.609375, acc 0.5051686381074169\n",
      "Epoch 62\n",
      "[62]Train loss 37563.375, acc 0.5025484625668449\n",
      "[62]Test loss 39270.7734375, acc 0.5290521099744246\n",
      "Epoch 63\n",
      "[63]Train loss 36782.01953125, acc 0.48395257724301854\n",
      "[63]Test loss 38978.484375, acc 0.5026742327365729\n",
      "Epoch 64\n",
      "[64]Train loss 36485.53515625, acc 0.4857251102655513\n",
      "[64]Test loss 39415.05078125, acc 0.4637284207161125\n",
      "Epoch 65\n",
      "[65]Train loss 36065.17578125, acc 0.4876458304767128\n",
      "[65]Test loss 43711.390625, acc 0.5110110294117647\n",
      "Epoch 66\n",
      "[66]Train loss 35722.19921875, acc 0.49583076237488005\n",
      "[66]Test loss 39171.0390625, acc 0.5154955242966752\n",
      "Epoch 67\n",
      "[67]Train loss 35426.875, acc 0.4878822158233923\n",
      "[67]Test loss 41979.7109375, acc 0.4715009590792839\n",
      "Epoch 68\n",
      "[68]Train loss 35019.078125, acc 0.46975445918460623\n",
      "[68]Test loss 39926.94140625, acc 0.4970412404092072\n",
      "Epoch 69\n",
      "[69]Train loss 34515.03515625, acc 0.4749363687782805\n",
      "[69]Test loss 39109.5234375, acc 0.4857344948849104\n",
      "Epoch 70\n",
      "[70]Train loss 34126.4375, acc 0.5005331168014991\n",
      "[70]Test loss 39114.7421875, acc 0.47897538363171355\n",
      "Epoch 71\n",
      "[71]Train loss 33764.1875, acc 0.5006034610814022\n",
      "[71]Test loss 38801.52734375, acc 0.48471387468030697\n",
      "Epoch 72\n",
      "[72]Train loss 33398.16796875, acc 0.48209113190730823\n",
      "[72]Test loss 38298.28515625, acc 0.5001598465473147\n",
      "Epoch 73\n",
      "[73]Train loss 32952.4609375, acc 0.4934444130216189\n",
      "[73]Test loss 38463.140625, acc 0.5093502237851663\n",
      "Epoch 74\n",
      "[74]Train loss 32733.974609375, acc 0.4944031662781663\n",
      "[74]Test loss 38760.82421875, acc 0.49505754475703323\n",
      "Epoch 75\n",
      "[75]Train loss 32088.9921875, acc 0.5012551276338041\n",
      "[75]Test loss 38500.06640625, acc 0.5152917199488491\n",
      "Epoch 76\n",
      "[76]Train loss 31734.36328125, acc 0.5029794551853374\n",
      "[76]Test loss 40696.2109375, acc 0.4999856138107418\n",
      "Epoch 77\n",
      "[77]Train loss 31527.841796875, acc 0.4910723467708762\n",
      "[77]Test loss 40180.609375, acc 0.45360054347826084\n",
      "Epoch 78\n",
      "[78]Train loss 30854.244140625, acc 0.4747831825037707\n",
      "[78]Test loss 38649.328125, acc 0.4843749999999999\n",
      "Epoch 79\n",
      "[79]Train loss 30309.572265625, acc 0.49244995200877556\n",
      "[79]Test loss 38573.26953125, acc 0.4682760549872122\n",
      "Epoch 80\n",
      "[80]Train loss 30118.533203125, acc 0.48779830259609674\n",
      "[80]Test loss 38126.12890625, acc 0.5047154731457801\n",
      "Epoch 81\n",
      "[81]Train loss 29968.677734375, acc 0.47458928938708356\n",
      "[81]Test loss 39447.37109375, acc 0.4665577046035805\n",
      "Epoch 82\n",
      "[82]Train loss 29001.3359375, acc 0.4921899995429406\n",
      "[82]Test loss 38229.48046875, acc 0.489088874680307\n",
      "Epoch 83\n",
      "[83]Train loss 28664.833984375, acc 0.4868088406691347\n",
      "[83]Test loss 38609.7890625, acc 0.49740648976982105\n",
      "Epoch 84\n",
      "[84]Train loss 28253.173828125, acc 0.4819361602449838\n",
      "[84]Test loss 38335.7890625, acc 0.4886133312020461\n",
      "Epoch 85\n",
      "[85]Train loss 27711.27734375, acc 0.49765828534210876\n",
      "[85]Test loss 39088.23046875, acc 0.4900415601023018\n",
      "Epoch 86\n",
      "[86]Train loss 27266.052734375, acc 0.4639676601992779\n",
      "[86]Test loss 38277.62109375, acc 0.43912803708439907\n",
      "Epoch 87\n",
      "[87]Train loss 26811.86328125, acc 0.48601362893642297\n",
      "[87]Test loss 40827.48046875, acc 0.4761988491048593\n",
      "Epoch 88\n",
      "[88]Train loss 26485.9140625, acc 0.49302448980757807\n",
      "[88]Test loss 39986.92578125, acc 0.5032968350383632\n",
      "Epoch 89\n",
      "[89]Train loss 25826.28125, acc 0.4859207887700535\n",
      "[89]Test loss 37848.9375, acc 0.5306873401534528\n",
      "Epoch 90\n",
      "[90]Train loss 25319.048828125, acc 0.5016686234517115\n",
      "[90]Test loss 40914.0703125, acc 0.49616608056265976\n",
      "Epoch 91\n",
      "[91]Train loss 24925.171875, acc 0.5036036267653914\n",
      "[91]Test loss 39467.08203125, acc 0.5239098465473147\n",
      "Epoch 92\n",
      "[92]Train loss 24192.115234375, acc 0.4673416860916861\n",
      "[92]Test loss 40119.53125, acc 0.4996123721227622\n",
      "Epoch 93\n",
      "[93]Train loss 23931.94921875, acc 0.4886224376114082\n",
      "[93]Test loss 39050.37890625, acc 0.49087915601023024\n",
      "Epoch 94\n",
      "[94]Train loss 23539.25, acc 0.499600073129485\n",
      "[94]Test loss 40064.609375, acc 0.4677405690537085\n",
      "Epoch 95\n",
      "[95]Train loss 22624.015625, acc 0.4848459853055441\n",
      "[95]Test loss 38799.64453125, acc 0.46639625959079295\n",
      "Epoch 96\n",
      "[96]Train loss 22237.8046875, acc 0.48902129324923443\n",
      "[96]Test loss 40152.84375, acc 0.49306425831202044\n",
      "Epoch 97\n",
      "[97]Train loss 21712.51171875, acc 0.48412575986105394\n",
      "[97]Test loss 42370.328125, acc 0.47938539002557556\n",
      "Epoch 98\n",
      "[98]Train loss 21184.818359375, acc 0.4574784896476073\n",
      "[98]Test loss 41046.421875, acc 0.4988467071611253\n",
      "Epoch 99\n",
      "[99]Train loss 20799.373046875, acc 0.4896658182503771\n",
      "[99]Test loss 38789.42578125, acc 0.5005490728900256\n",
      "Epoch 100\n",
      "[100]Train loss 20029.23828125, acc 0.4709831630787513\n",
      "[100]Test loss 40705.96875, acc 0.4721227621483376\n",
      "Epoch 101\n",
      "[101]Train loss 19490.87109375, acc 0.4954283359614242\n",
      "[101]Test loss 41368.9765625, acc 0.49782928388746794\n",
      "Epoch 102\n",
      "[102]Train loss 19102.908203125, acc 0.49849848884775355\n",
      "[102]Test loss 42511.23828125, acc 0.5014801790281329\n",
      "Epoch 103\n",
      "[103]Train loss 18369.859375, acc 0.4768681583938936\n",
      "[103]Test loss 39401.22265625, acc 0.4771435421994884\n",
      "Epoch 104\n",
      "[104]Train loss 18288.765625, acc 0.46873000365647427\n",
      "[104]Test loss 39662.6328125, acc 0.5110126278772378\n",
      "Epoch 105\n",
      "[105]Train loss 17291.78125, acc 0.495644010809452\n",
      "[105]Test loss 42513.078125, acc 0.4895356457800511\n",
      "Epoch 106\n",
      "[106]Train loss 16817.626953125, acc 0.4927909610814023\n",
      "[106]Test loss 40728.05859375, acc 0.5129491687979539\n",
      "Epoch 107\n",
      "[107]Train loss 16505.46875, acc 0.4925292232277528\n",
      "[107]Test loss 39981.01953125, acc 0.47655690537084405\n",
      "Epoch 108\n",
      "[108]Train loss 15865.236328125, acc 0.45907748297454176\n",
      "[108]Test loss 41262.30078125, acc 0.45269820971867014\n",
      "Epoch 109\n",
      "[109]Train loss 15437.99609375, acc 0.4688374840029252\n",
      "[109]Test loss 40543.3984375, acc 0.5119397378516624\n",
      "Epoch 110\n",
      "[110]Train loss 14851.5390625, acc 0.48501595422551314\n",
      "[110]Test loss 40387.140625, acc 0.5024952046035808\n",
      "Epoch 111\n",
      "[111]Train loss 14286.5029296875, acc 0.4510589492207139\n",
      "[111]Test loss 41228.859375, acc 0.39185901534526857\n",
      "Epoch 112\n",
      "[112]Train loss 13706.951171875, acc 0.458755041935189\n",
      "[112]Test loss 39641.55078125, acc 0.4942351342710996\n",
      "Epoch 113\n",
      "[113]Train loss 13428.5400390625, acc 0.4961003559349149\n",
      "[113]Test loss 42386.2109375, acc 0.48985933503836304\n",
      "Epoch 114\n",
      "[114]Train loss 12970.92578125, acc 0.48028610482654616\n",
      "[114]Test loss 41692.91796875, acc 0.49844069693094634\n",
      "Epoch 115\n",
      "[115]Train loss 12429.6552734375, acc 0.4999753616481557\n",
      "[115]Test loss 41247.1796875, acc 0.4932049232736574\n",
      "Epoch 116\n",
      "[116]Train loss 11978.5126953125, acc 0.4848609825631885\n",
      "[116]Test loss 41925.8125, acc 0.4873505434782608\n",
      "Epoch 117\n",
      "[117]Train loss 11640.02734375, acc 0.48845425407925414\n",
      "[117]Test loss 42362.234375, acc 0.5115201406649618\n",
      "Epoch 118\n",
      "[118]Train loss 11083.6689453125, acc 0.482878487933635\n",
      "[118]Test loss 43035.93359375, acc 0.3783895460358056\n",
      "Epoch 119\n",
      "[119]Train loss 10658.0361328125, acc 0.4740654566022214\n",
      "[119]Test loss 42927.59375, acc 0.4514977621483376\n",
      "Epoch 120\n",
      "[120]Train loss 10161.7646484375, acc 0.46622796117281406\n",
      "[120]Test loss 42722.9296875, acc 0.3274128836317136\n",
      "Epoch 121\n",
      "[121]Train loss 9707.76171875, acc 0.4245405840074957\n",
      "[121]Test loss 43919.94140625, acc 0.49276454603580566\n",
      "Epoch 122\n",
      "[122]Train loss 9573.46484375, acc 0.4944253050870697\n",
      "[122]Test loss 44508.09375, acc 0.510069533248082\n",
      "Epoch 123\n",
      "[123]Train loss 9050.796875, acc 0.46640328625622746\n",
      "[123]Test loss 43058.328125, acc 0.4922866048593351\n",
      "Epoch 124\n",
      "[124]Train loss 8748.326171875, acc 0.4919464726450022\n",
      "[124]Test loss 43272.48828125, acc 0.4552054028132992\n",
      "Epoch 125\n",
      "[125]Train loss 8375.421875, acc 0.49135443804561457\n",
      "[125]Test loss 43811.88671875, acc 0.4949536445012787\n",
      "Epoch 126\n",
      "[126]Train loss 7882.103515625, acc 0.4639148127199598\n",
      "[126]Test loss 41870.85546875, acc 0.48874600383631717\n",
      "Epoch 127\n",
      "[127]Train loss 7898.9462890625, acc 0.4883935508935509\n",
      "[127]Test loss 44283.80859375, acc 0.47679267902813294\n",
      "Epoch 128\n",
      "[128]Train loss 7488.14404296875, acc 0.49793680584121763\n",
      "[128]Test loss 44549.9609375, acc 0.4761253196930946\n",
      "Epoch 129\n",
      "[129]Train loss 7049.27392578125, acc 0.4688956876456876\n",
      "[129]Test loss 46585.140625, acc 0.41585278132992326\n",
      "Epoch 130\n",
      "[130]Train loss 6727.82470703125, acc 0.44274332693450336\n",
      "[130]Test loss 42971.203125, acc 0.48039721867007673\n",
      "Epoch 131\n",
      "[131]Train loss 6467.5390625, acc 0.475689802436126\n",
      "[131]Test loss 43817.875, acc 0.5015329283887467\n",
      "Epoch 132\n",
      "[132]Train loss 6207.44384765625, acc 0.5026255913204443\n",
      "[132]Test loss 45921.578125, acc 0.5051934143222505\n",
      "Epoch 133\n",
      "[133]Train loss 5901.78662109375, acc 0.49105663535810584\n",
      "[133]Test loss 45854.6640625, acc 0.5050383631713555\n",
      "Epoch 134\n",
      "[134]Train loss 5630.06201171875, acc 0.49754437759952463\n",
      "[134]Test loss 42620.609375, acc 0.5281609654731457\n",
      "Epoch 135\n",
      "[135]Train loss 5454.4716796875, acc 0.4761629302070478\n",
      "[135]Test loss 44873.78515625, acc 0.47033168158567773\n",
      "Epoch 136\n",
      "[136]Train loss 5318.650390625, acc 0.39195618515471464\n",
      "[136]Test loss 44843.8359375, acc 0.47403132992327374\n",
      "Epoch 137\n",
      "[137]Train loss 4976.46240234375, acc 0.4972597867818455\n",
      "[137]Test loss 46195.71484375, acc 0.4502805306905371\n",
      "Epoch 138\n",
      "[138]Train loss 4817.62109375, acc 0.45505393299510954\n",
      "[138]Test loss 43409.26171875, acc 0.4841719948849106\n",
      "Epoch 139\n",
      "[139]Train loss 4714.03857421875, acc 0.47584905902920616\n",
      "[139]Test loss 45475.40625, acc 0.4391759910485934\n",
      "Epoch 140\n",
      "[140]Train loss 4371.40771484375, acc 0.49105127919466146\n",
      "[140]Test loss 44582.11328125, acc 0.5138682864450127\n",
      "Epoch 141\n",
      "[141]Train loss 4279.271484375, acc 0.5062092217423101\n",
      "[141]Test loss 45093.96484375, acc 0.5147826086956522\n",
      "Epoch 142\n",
      "[142]Train loss 3951.379150390625, acc 0.4904870966451849\n",
      "[142]Test loss 45112.65625, acc 0.49755514705882353\n",
      "Epoch 143\n",
      "[143]Train loss 3789.949462890625, acc 0.4940285919146214\n",
      "[143]Test loss 46072.8359375, acc 0.5042287404092072\n",
      "Epoch 144\n",
      "[144]Train loss 3640.208984375, acc 0.4644350747291924\n",
      "[144]Test loss 44213.234375, acc 0.5189474104859334\n",
      "Epoch 145\n",
      "[145]Train loss 3494.233154296875, acc 0.5036375491338727\n",
      "[145]Test loss 47130.078125, acc 0.4860358056265985\n",
      "Epoch 146\n",
      "[146]Train loss 3319.223876953125, acc 0.4836279937382879\n",
      "[146]Test loss 44798.51953125, acc 0.5125135869565217\n",
      "Epoch 147\n",
      "[147]Train loss 3080.750732421875, acc 0.458442956145162\n",
      "[147]Test loss 43322.5, acc 0.514760230179028\n",
      "Epoch 148\n",
      "[148]Train loss 2978.3369140625, acc 0.4869081082316377\n",
      "[148]Test loss 44768.87890625, acc 0.47305306905370836\n",
      "Epoch 149\n",
      "[149]Train loss 2834.6318359375, acc 0.4788535096439508\n",
      "[149]Test loss 44593.6484375, acc 0.4670028772378516\n",
      "Epoch 150\n",
      "[150]Train loss 2674.263916015625, acc 0.4951969497006263\n",
      "[150]Test loss 47569.73046875, acc 0.5244932864450128\n",
      "Epoch 151\n",
      "[151]Train loss 2544.629638671875, acc 0.4466415426893368\n",
      "[151]Test loss 47178.82421875, acc 0.4651662404092072\n",
      "Epoch 152\n",
      "[152]Train loss 2509.1025390625, acc 0.4559923328305681\n",
      "[152]Test loss 44745.42578125, acc 0.5016032608695652\n",
      "Epoch 153\n",
      "[153]Train loss 2313.9453125, acc 0.47451394602129904\n",
      "[153]Test loss 45583.53515625, acc 0.5111636828644502\n",
      "Epoch 154\n",
      "[154]Train loss 2256.198974609375, acc 0.4731127736642442\n",
      "[154]Test loss 46200.82421875, acc 0.43769261508951407\n",
      "Epoch 155\n",
      "[155]Train loss 2088.499755859375, acc 0.4514977975455916\n",
      "[155]Test loss 45379.10546875, acc 0.3991967710997443\n",
      "Epoch 156\n",
      "[156]Train loss 1995.36279296875, acc 0.4299510232414644\n",
      "[156]Test loss 46022.125, acc 0.4936988491048594\n",
      "Epoch 157\n",
      "[157]Train loss 1909.42236328125, acc 0.4601887083504731\n",
      "[157]Test loss 45282.7265625, acc 0.47943094629156\n",
      "Epoch 158\n",
      "[158]Train loss 1806.8134765625, acc 0.47838216726084376\n",
      "[158]Test loss 45728.5234375, acc 0.4842215473145779\n",
      "Epoch 159\n",
      "[159]Train loss 1747.6416015625, acc 0.47811114539055727\n",
      "[159]Test loss 45400.30859375, acc 0.494968030690537\n",
      "Epoch 160\n",
      "[160]Train loss 1687.13671875, acc 0.45494895219159925\n",
      "[160]Test loss 45106.2109375, acc 0.5229267902813299\n",
      "Epoch 161\n",
      "[161]Train loss 1603.8804931640625, acc 0.4806485385529503\n",
      "[161]Test loss 45486.11328125, acc 0.48919357416879783\n",
      "Epoch 162\n",
      "[162]Train loss 1496.64208984375, acc 0.43204849684629093\n",
      "[162]Test loss 46412.3515625, acc 0.4206281969309463\n",
      "Epoch 163\n",
      "[163]Train loss 1437.8385009765625, acc 0.45530103066867766\n",
      "[163]Test loss 46381.98828125, acc 0.48839593989769814\n",
      "Epoch 164\n",
      "[164]Train loss 1357.3726806640625, acc 0.48209720222587876\n",
      "[164]Test loss 46389.734375, acc 0.5142455242966752\n",
      "Epoch 165\n",
      "[165]Train loss 1300.7197265625, acc 0.5098149909730791\n",
      "[165]Test loss 46888.42578125, acc 0.523197730179028\n",
      "Epoch 166\n",
      "[166]Train loss 1237.816650390625, acc 0.4884967463092464\n",
      "[166]Test loss 46202.0, acc 0.4832760549872122\n",
      "Epoch 167\n",
      "[167]Train loss 1184.17529296875, acc 0.47930735522647294\n",
      "[167]Test loss 46278.0234375, acc 0.4700271739130435\n",
      "Epoch 168\n",
      "[168]Train loss 1164.80810546875, acc 0.44647835824306414\n",
      "[168]Test loss 46024.1640625, acc 0.4257624680306906\n",
      "Epoch 169\n",
      "[169]Train loss 1078.35791015625, acc 0.45367597067964716\n",
      "[169]Test loss 46180.015625, acc 0.4785262148337595\n",
      "Epoch 170\n",
      "[170]Train loss 1031.955810546875, acc 0.49170830191050785\n",
      "[170]Test loss 45948.453125, acc 0.4852925191815858\n",
      "Epoch 171\n",
      "[171]Train loss 1005.1627807617188, acc 0.48411861830979486\n",
      "[171]Test loss 45595.0546875, acc 0.48444373401534535\n",
      "Epoch 172\n",
      "[172]Train loss 952.18505859375, acc 0.4714712881073175\n",
      "[172]Test loss 45772.171875, acc 0.4655011189258312\n",
      "Epoch 173\n",
      "[173]Train loss 902.322509765625, acc 0.46380340452031626\n",
      "[173]Test loss 45813.2109375, acc 0.45660326086956515\n",
      "Epoch 174\n",
      "[174]Train loss 870.0064086914062, acc 0.4463269573563691\n",
      "[174]Test loss 47411.37890625, acc 0.35767583120204605\n",
      "Epoch 175\n",
      "[175]Train loss 839.2109375, acc 0.4219874794323324\n",
      "[175]Test loss 47563.23828125, acc 0.36727141943734015\n",
      "Epoch 176\n",
      "[176]Train loss 806.0538940429688, acc 0.4320802767493945\n",
      "[176]Test loss 45641.6640625, acc 0.48832400895140665\n",
      "Epoch 177\n",
      "[177]Train loss 783.6091918945312, acc 0.4930680532702592\n",
      "[177]Test loss 47803.82421875, acc 0.42153932225063934\n",
      "Epoch 178\n",
      "[178]Train loss 743.3418579101562, acc 0.4357617464235112\n",
      "[178]Test loss 45628.0703125, acc 0.3322066815856777\n",
      "Epoch 179\n",
      "[179]Train loss 701.47607421875, acc 0.4206705773801362\n",
      "[179]Test loss 45287.30859375, acc 0.4559127237851662\n",
      "Epoch 180\n",
      "[180]Train loss 675.7088623046875, acc 0.5009437559989031\n",
      "[180]Test loss 45475.63671875, acc 0.45524296675191817\n",
      "Epoch 181\n",
      "[181]Train loss 662.7745971679688, acc 0.49112376593994234\n",
      "[181]Test loss 46860.65625, acc 0.49223865089514074\n",
      "Epoch 182\n",
      "[182]Train loss 625.726806640625, acc 0.4788038758626994\n",
      "[182]Test loss 48146.32421875, acc 0.5057608695652174\n",
      "Epoch 183\n",
      "[183]Train loss 600.1250610351562, acc 0.4952612236619589\n",
      "[183]Test loss 45655.03515625, acc 0.4821635230179029\n",
      "Epoch 184\n",
      "[184]Train loss 570.3670043945312, acc 0.46923205470999585\n",
      "[184]Test loss 46024.98046875, acc 0.4735342071611253\n",
      "Epoch 185\n",
      "[185]Train loss 552.0154418945312, acc 0.48955369589560765\n",
      "[185]Test loss 45770.60546875, acc 0.5241879795396419\n",
      "Epoch 186\n",
      "[186]Train loss 523.4371337890625, acc 0.4563911884683943\n",
      "[186]Test loss 45603.37109375, acc 0.47335677749360616\n",
      "Epoch 187\n",
      "[187]Train loss 507.9535217285156, acc 0.4980403583344759\n",
      "[187]Test loss 46359.703125, acc 0.41912563938618924\n",
      "Epoch 188\n",
      "[188]Train loss 486.8201599121094, acc 0.4759776069518717\n",
      "[188]Test loss 46192.1640625, acc 0.5242766943734014\n",
      "Epoch 189\n",
      "[189]Train loss 486.8803405761719, acc 0.4769024378399378\n",
      "[189]Test loss 49344.01171875, acc 0.4889985613810741\n",
      "Epoch 190\n",
      "[190]Train loss 448.1177062988281, acc 0.45078435657479776\n",
      "[190]Test loss 45949.4453125, acc 0.4506617647058823\n",
      "Epoch 191\n",
      "[191]Train loss 428.04254150390625, acc 0.4430632684309156\n",
      "[191]Test loss 45376.734375, acc 0.46576326726342715\n",
      "Epoch 192\n",
      "[192]Train loss 414.4784240722656, acc 0.471823366584396\n",
      "[192]Test loss 45336.8203125, acc 0.41189737851662406\n",
      "Epoch 193\n",
      "[193]Train loss 397.25982666015625, acc 0.4210058732117556\n",
      "[193]Test loss 45614.37890625, acc 0.4716576086956521\n",
      "Epoch 194\n",
      "[194]Train loss 383.2394714355469, acc 0.4855665678275972\n",
      "[194]Test loss 45825.5859375, acc 0.4625239769820973\n",
      "Epoch 195\n",
      "[195]Train loss 361.56658935546875, acc 0.4445740635997989\n",
      "[195]Test loss 45161.80078125, acc 0.5230770460358056\n",
      "Epoch 196\n",
      "[196]Train loss 343.77667236328125, acc 0.4967288124457242\n",
      "[196]Test loss 44582.0390625, acc 0.3147226662404092\n",
      "Epoch 197\n",
      "[197]Train loss 340.10595703125, acc 0.4598619823803647\n",
      "[197]Test loss 45919.2109375, acc 0.49940696930946293\n",
      "Epoch 198\n",
      "[198]Train loss 324.0690002441406, acc 0.4836151389460214\n",
      "[198]Test loss 45432.203125, acc 0.47802749360613817\n",
      "Epoch 199\n",
      "[199]Train loss 302.7350769042969, acc 0.4747156948443714\n",
      "[199]Test loss 45082.2265625, acc 0.49497202685422\n",
      "Epoch 200\n",
      "[200]Train loss 289.5995788574219, acc 0.4725821564056858\n",
      "[200]Test loss 44450.734375, acc 0.47188299232736575\n",
      "Epoch 201\n",
      "[201]Train loss 285.736083984375, acc 0.49606464817861873\n",
      "[201]Test loss 44396.93359375, acc 0.5048825127877239\n",
      "Epoch 202\n",
      "[202]Train loss 277.47100830078125, acc 0.4767106871886283\n",
      "[202]Test loss 45409.9453125, acc 0.4910541879795397\n",
      "Epoch 203\n",
      "[203]Train loss 267.9881286621094, acc 0.47721880856072035\n",
      "[203]Test loss 44536.40625, acc 0.48365089514066495\n",
      "Epoch 204\n",
      "[204]Train loss 251.46875, acc 0.4278814016865487\n",
      "[204]Test loss 44973.21875, acc 0.43948929028133\n",
      "Epoch 205\n",
      "[205]Train loss 242.42361450195312, acc 0.47549805178481663\n",
      "[205]Test loss 44889.921875, acc 0.5177030051150895\n",
      "Epoch 206\n",
      "[206]Train loss 227.9112091064453, acc 0.5063131313131313\n",
      "[206]Test loss 44672.109375, acc 0.3317974744245525\n",
      "Epoch 207\n",
      "[207]Train loss 224.00039672851562, acc 0.42840773401435167\n",
      "[207]Test loss 44975.109375, acc 0.42386668797953964\n",
      "Epoch 208\n",
      "[208]Train loss 208.77395629882812, acc 0.4551524864024864\n",
      "[208]Test loss 45127.62890625, acc 0.46735453964194373\n",
      "Epoch 209\n",
      "[209]Train loss 204.98135375976562, acc 0.4780165198363728\n",
      "[209]Test loss 44160.57421875, acc 0.47440617007672636\n",
      "Epoch 210\n",
      "[210]Train loss 203.98321533203125, acc 0.49702054481466246\n",
      "[210]Test loss 44576.09375, acc 0.49603340792838857\n",
      "Epoch 211\n",
      "[211]Train loss 212.1352996826172, acc 0.4914954836829837\n",
      "[211]Test loss 44540.19921875, acc 0.47526614450127874\n",
      "Epoch 212\n",
      "[212]Train loss 196.63299560546875, acc 0.48123378867864164\n",
      "[212]Test loss 44944.24609375, acc 0.4639713874680308\n",
      "Epoch 213\n",
      "[213]Train loss 187.95506286621094, acc 0.46791193895973315\n",
      "[213]Test loss 45433.23828125, acc 0.49406489769820977\n",
      "Epoch 214\n",
      "[214]Train loss 163.19386291503906, acc 0.4959835915718268\n",
      "[214]Test loss 44626.51171875, acc 0.5008184143222507\n",
      "Epoch 215\n",
      "[215]Train loss 156.1509246826172, acc 0.47746447792403673\n",
      "[215]Test loss 44611.890625, acc 0.4688251278772379\n",
      "Epoch 216\n",
      "[216]Train loss 149.6698455810547, acc 0.4703404234654235\n",
      "[216]Test loss 44000.75390625, acc 0.4955274936061381\n",
      "Epoch 217\n",
      "[217]Train loss 152.41053771972656, acc 0.48941550687874225\n",
      "[217]Test loss 46837.01171875, acc 0.5015744884910487\n",
      "Epoch 218\n",
      "[218]Train loss 152.29981994628906, acc 0.4884496120709356\n",
      "[218]Test loss 45977.88671875, acc 0.4501054987212276\n",
      "Epoch 219\n",
      "[219]Train loss 143.6566619873047, acc 0.44554210087298324\n",
      "[219]Test loss 45757.11328125, acc 0.5184454923273657\n",
      "Epoch 220\n",
      "[220]Train loss 134.69561767578125, acc 0.48605255039078576\n",
      "[220]Test loss 44506.3359375, acc 0.5067231457800512\n",
      "Epoch 221\n",
      "[221]Train loss 126.62149810791016, acc 0.4615052533251063\n",
      "[221]Test loss 44034.359375, acc 0.34062180306905376\n",
      "Epoch 222\n",
      "[222]Train loss 129.3480682373047, acc 0.416939831002331\n",
      "[222]Test loss 45937.4296875, acc 0.5093126598465473\n",
      "Epoch 223\n",
      "[223]Train loss 112.76056671142578, acc 0.489212329745418\n",
      "[223]Test loss 44539.60546875, acc 0.499047314578005\n",
      "Epoch 224\n",
      "[224]Train loss 106.16325378417969, acc 0.47357304664289956\n",
      "[224]Test loss 44415.71875, acc 0.48596227621483373\n",
      "Epoch 225\n",
      "[225]Train loss 103.21168518066406, acc 0.4739983260203849\n",
      "[225]Test loss 44405.3046875, acc 0.3724936061381074\n",
      "Epoch 226\n",
      "[226]Train loss 100.60394287109375, acc 0.4142899584076054\n",
      "[226]Test loss 44290.140625, acc 0.44249040920716115\n",
      "Epoch 227\n",
      "[227]Train loss 104.8997802734375, acc 0.49513089035147856\n",
      "[227]Test loss 46076.15625, acc 0.502988331202046\n",
      "Epoch 228\n",
      "[228]Train loss 119.35557556152344, acc 0.47682852278440524\n",
      "[228]Test loss 44328.62109375, acc 0.4611213235294118\n",
      "Epoch 229\n",
      "[229]Train loss 142.0839385986328, acc 0.4814419648978472\n",
      "[229]Test loss 44579.078125, acc 0.4863930626598465\n",
      "Epoch 230\n",
      "[230]Train loss 138.18565368652344, acc 0.4935372531879884\n",
      "[230]Test loss 44493.828125, acc 0.38041879795396427\n",
      "Epoch 231\n",
      "[231]Train loss 140.42062377929688, acc 0.4670510249554367\n",
      "[231]Test loss 45037.13671875, acc 0.49974984015345275\n",
      "Epoch 232\n",
      "[232]Train loss 93.27758026123047, acc 0.5062838509529687\n",
      "[232]Test loss 44119.2890625, acc 0.49393222506393863\n",
      "Epoch 233\n",
      "[233]Train loss 79.98173522949219, acc 0.4612652972027972\n",
      "[233]Test loss 44344.796875, acc 0.5185901534526854\n",
      "Epoch 234\n",
      "[234]Train loss 72.67193603515625, acc 0.49427926036381914\n",
      "[234]Test loss 44190.13671875, acc 0.4548113810741688\n",
      "Epoch 235\n",
      "[235]Train loss 70.65399169921875, acc 0.43318328934137756\n",
      "[235]Test loss 44221.125, acc 0.4880266943734014\n",
      "Epoch 236\n",
      "[236]Train loss 69.62419891357422, acc 0.48110024167009463\n",
      "[236]Test loss 44227.375, acc 0.5261141304347826\n",
      "Epoch 237\n",
      "[237]Train loss 66.01376342773438, acc 0.4985031308560719\n",
      "[237]Test loss 44986.109375, acc 0.44880195012787727\n",
      "Epoch 238\n",
      "[238]Train loss 63.82448959350586, acc 0.48755120492252857\n",
      "[238]Test loss 44393.42578125, acc 0.38322730179028136\n",
      "Epoch 239\n",
      "[239]Train loss 62.33378601074219, acc 0.45239227684080624\n",
      "[239]Test loss 44595.96875, acc 0.4600895140664962\n",
      "Epoch 240\n",
      "[240]Train loss 61.45056915283203, acc 0.47325203391379866\n",
      "[240]Test loss 44070.640625, acc 0.46811700767263426\n",
      "Epoch 241\n",
      "[241]Train loss 59.71117401123047, acc 0.46742845593948534\n",
      "[241]Test loss 45255.08984375, acc 0.500350063938619\n",
      "Epoch 242\n",
      "[242]Train loss 95.13711547851562, acc 0.49410286404771697\n",
      "[242]Test loss 44525.578125, acc 0.4758224104859335\n",
      "Epoch 243\n",
      "[243]Train loss 75.685302734375, acc 0.48805825506193157\n",
      "[243]Test loss 43869.8359375, acc 0.49799792199488485\n",
      "Epoch 244\n",
      "[244]Train loss 74.9145736694336, acc 0.40053961561314505\n",
      "[244]Test loss 46312.41015625, acc 0.4831593670076726\n",
      "Epoch 245\n",
      "[245]Train loss 61.46332550048828, acc 0.46851111511037974\n",
      "[245]Test loss 44186.3203125, acc 0.5057296994884911\n",
      "Epoch 246\n",
      "[246]Train loss 51.019615173339844, acc 0.485649052744641\n",
      "[246]Test loss 44169.875, acc 0.4168470268542199\n",
      "Epoch 247\n",
      "[247]Train loss 50.34775161743164, acc 0.4795987305178482\n",
      "[247]Test loss 44558.08984375, acc 0.5290401214833759\n",
      "Epoch 248\n",
      "[248]Train loss 43.02057647705078, acc 0.4880289747017689\n",
      "[248]Test loss 44311.671875, acc 0.5343334398976981\n",
      "Epoch 249\n",
      "[249]Train loss 42.04133605957031, acc 0.4954372629004983\n",
      "[249]Test loss 44426.8046875, acc 0.47689418158567753\n",
      "Epoch 250\n",
      "[250]Train loss 39.779930114746094, acc 0.48709628810731737\n",
      "[250]Test loss 44152.0703125, acc 0.42933423913043484\n",
      "Epoch 251\n",
      "[251]Train loss 42.20414733886719, acc 0.47960372960372966\n",
      "[251]Test loss 44502.921875, acc 0.5154036125319694\n",
      "Epoch 252\n",
      "[252]Train loss 43.789772033691406, acc 0.4810149001325473\n",
      "[252]Test loss 44707.99609375, acc 0.4496675191815857\n",
      "Epoch 253\n",
      "[253]Train loss 68.2394027709961, acc 0.4690106666209607\n",
      "[253]Test loss 44064.8046875, acc 0.3599224744245524\n",
      "Epoch 254\n",
      "[254]Train loss 60.32225036621094, acc 0.42375751291192476\n",
      "[254]Test loss 45528.12890625, acc 0.48221547314577995\n",
      "Epoch 255\n",
      "[255]Train loss 54.0611572265625, acc 0.48282635460944284\n",
      "[255]Test loss 43698.7890625, acc 0.5198601342710999\n",
      "Epoch 256\n",
      "[256]Train loss 65.09909057617188, acc 0.45549492378536505\n",
      "[256]Test loss 44020.2890625, acc 0.447488810741688\n",
      "Epoch 257\n",
      "[257]Train loss 53.185123443603516, acc 0.4843442913295854\n",
      "[257]Test loss 45790.953125, acc 0.5184382992327367\n",
      "Epoch 258\n",
      "[258]Train loss 43.415897369384766, acc 0.5044545425979249\n",
      "[258]Test loss 44266.359375, acc 0.47646499360613814\n",
      "Epoch 259\n",
      "[259]Train loss 39.03001022338867, acc 0.5043292083733261\n",
      "[259]Test loss 44705.00390625, acc 0.4640393222506394\n",
      "Epoch 260\n",
      "[260]Train loss 37.21506881713867, acc 0.48925589320809904\n",
      "[260]Test loss 46189.30078125, acc 0.4755115089514067\n",
      "Epoch 261\n",
      "[261]Train loss 39.53518295288086, acc 0.47116277309292015\n",
      "[261]Test loss 44348.89453125, acc 0.45890265345268544\n",
      "Epoch 262\n",
      "[262]Train loss 37.362361907958984, acc 0.5034797208510445\n",
      "[262]Test loss 47156.94921875, acc 0.5126358695652173\n",
      "Epoch 263\n",
      "[263]Train loss 35.50670623779297, acc 0.5005923916769505\n",
      "[263]Test loss 46517.33984375, acc 0.48604939258312013\n",
      "Epoch 264\n",
      "[264]Train loss 59.37173843383789, acc 0.45933565005256183\n",
      "[264]Test loss 45004.96484375, acc 0.530108695652174\n",
      "Epoch 265\n",
      "[265]Train loss 89.48370361328125, acc 0.4783482448923625\n",
      "[265]Test loss 44007.10546875, acc 0.4236117327365729\n",
      "Epoch 266\n",
      "[266]Train loss 80.1970443725586, acc 0.4496306389688743\n",
      "[266]Test loss 43913.3203125, acc 0.5063107416879797\n",
      "Epoch 267\n",
      "[267]Train loss 43.19173049926758, acc 0.47117062879930527\n",
      "[267]Test loss 44973.921875, acc 0.39604699488491046\n",
      "Epoch 268\n",
      "[268]Train loss 43.03287124633789, acc 0.44008095662507424\n",
      "[268]Test loss 44252.3515625, acc 0.5015089514066496\n",
      "Epoch 269\n",
      "[269]Train loss 38.614810943603516, acc 0.48186581596508077\n",
      "[269]Test loss 42887.11328125, acc 0.3349168797953964\n",
      "Epoch 270\n",
      "[270]Train loss 39.171504974365234, acc 0.4254354203802734\n",
      "[270]Test loss 45198.7265625, acc 0.5046331521739129\n",
      "Epoch 271\n",
      "[271]Train loss 43.310935974121094, acc 0.49380363304995667\n",
      "[271]Test loss 44392.75, acc 0.4395764066496164\n",
      "Epoch 272\n",
      "[272]Train loss 64.64777374267578, acc 0.36928497360482654\n",
      "[272]Test loss 45379.66015625, acc 0.41755195012787727\n",
      "Epoch 273\n",
      "[273]Train loss 51.18595504760742, acc 0.45904641722656425\n",
      "[273]Test loss 44448.671875, acc 0.493156969309463\n",
      "Epoch 274\n",
      "[274]Train loss 38.430824279785156, acc 0.5062759952465835\n",
      "[274]Test loss 43568.953125, acc 0.46349184782608693\n",
      "Epoch 275\n",
      "[275]Train loss 32.557159423828125, acc 0.48318343217240267\n",
      "[275]Test loss 44670.28515625, acc 0.5271011828644501\n",
      "Epoch 276\n",
      "[276]Train loss 26.520164489746094, acc 0.4685686045980163\n",
      "[276]Test loss 44412.4453125, acc 0.5003372762148337\n",
      "Epoch 277\n",
      "[277]Train loss 23.463285446166992, acc 0.47236398201471735\n",
      "[277]Test loss 45690.94921875, acc 0.4889242327365729\n",
      "Epoch 278\n",
      "[278]Train loss 23.74585723876953, acc 0.5098064211115683\n",
      "[278]Test loss 44538.62109375, acc 0.45334958439897705\n",
      "Epoch 279\n",
      "[279]Train loss 25.134645462036133, acc 0.49121553487362324\n",
      "[279]Test loss 44470.4453125, acc 0.46390265345268544\n",
      "Epoch 280\n",
      "[280]Train loss 23.012731552124023, acc 0.47173016934046347\n",
      "[280]Test loss 44068.609375, acc 0.5137531969309463\n",
      "Epoch 281\n",
      "[281]Train loss 21.35211181640625, acc 0.5018553750171397\n",
      "[281]Test loss 45794.19140625, acc 0.5146091751918159\n",
      "Epoch 282\n",
      "[282]Train loss 19.644287109375, acc 0.4919543283513872\n",
      "[282]Test loss 44171.8203125, acc 0.4807752557544757\n",
      "Epoch 283\n",
      "[283]Train loss 16.910215377807617, acc 0.4777508541295306\n",
      "[283]Test loss 44587.62109375, acc 0.4733703644501278\n",
      "Epoch 284\n",
      "[284]Train loss 19.2233943939209, acc 0.4732048996754878\n",
      "[284]Test loss 44262.14453125, acc 0.48263826726342707\n",
      "Epoch 285\n",
      "[285]Train loss 29.814504623413086, acc 0.4778115573152339\n",
      "[285]Test loss 44604.55859375, acc 0.3085054347826087\n",
      "Epoch 286\n",
      "[286]Train loss 40.33774948120117, acc 0.4522105243612597\n",
      "[286]Test loss 44352.140625, acc 0.47466432225063937\n",
      "Epoch 287\n",
      "[287]Train loss 36.35232925415039, acc 0.48700951825951816\n",
      "[287]Test loss 44243.375, acc 0.5027653452685423\n",
      "Epoch 288\n",
      "[288]Train loss 58.45502853393555, acc 0.5041263883175648\n",
      "[288]Test loss 44225.6796875, acc 0.3707217071611253\n",
      "Epoch 289\n",
      "[289]Train loss 51.37606430053711, acc 0.4764278817587642\n",
      "[289]Test loss 44164.89453125, acc 0.4820979859335039\n",
      "Epoch 290\n",
      "[290]Train loss 45.72216796875, acc 0.5027437839937839\n",
      "[290]Test loss 44778.609375, acc 0.5151654411764706\n",
      "Epoch 291\n",
      "[291]Train loss 33.96601867675781, acc 0.5021046151560858\n",
      "[291]Test loss 44403.3203125, acc 0.5007568734015344\n",
      "Epoch 292\n",
      "[292]Train loss 34.371585845947266, acc 0.47967693050413646\n",
      "[292]Test loss 43720.625, acc 0.503514226342711\n",
      "Epoch 293\n",
      "[293]Train loss 29.011978149414062, acc 0.45410160713469544\n",
      "[293]Test loss 44747.3359375, acc 0.4853732416879795\n",
      "Epoch 294\n",
      "[294]Train loss 26.45096778869629, acc 0.47729807977969746\n",
      "[294]Test loss 45065.75, acc 0.5050471547314578\n",
      "Epoch 295\n",
      "[295]Train loss 26.285972595214844, acc 0.49695234300013713\n",
      "[295]Test loss 43654.7265625, acc 0.5067535166240409\n",
      "Epoch 296\n",
      "[296]Train loss 22.173206329345703, acc 0.4525950968965675\n",
      "[296]Test loss 44091.94140625, acc 0.3097234654731458\n",
      "Epoch 297\n",
      "[297]Train loss 23.647979736328125, acc 0.4444373028931852\n",
      "[297]Test loss 44554.9453125, acc 0.3948505434782609\n",
      "Epoch 298\n",
      "[298]Train loss 25.652795791625977, acc 0.42798066924905165\n",
      "[298]Test loss 44194.96484375, acc 0.49039482097186704\n",
      "Epoch 299\n",
      "[299]Train loss 21.352651596069336, acc 0.49751616847205066\n",
      "[299]Test loss 46081.1796875, acc 0.4803868286445012\n",
      "Epoch 300\n",
      "[300]Train loss 36.47846984863281, acc 0.4995325854700855\n",
      "[300]Test loss 45568.171875, acc 0.40742886828644503\n",
      "Epoch 301\n",
      "[301]Train loss 61.66987609863281, acc 0.46446221262397736\n",
      "[301]Test loss 44331.984375, acc 0.46040680946291557\n",
      "Epoch 302\n",
      "[302]Train loss 57.17576599121094, acc 0.5093325791855204\n",
      "[302]Test loss 43436.171875, acc 0.5201118925831202\n",
      "Epoch 303\n",
      "[303]Train loss 30.311803817749023, acc 0.46268360928287405\n",
      "[303]Test loss 43314.3984375, acc 0.5156937340153451\n",
      "Epoch 304\n",
      "[304]Train loss 20.73447608947754, acc 0.4671356523378582\n",
      "[304]Test loss 43888.921875, acc 0.44158088235294113\n",
      "Epoch 305\n",
      "[305]Train loss 16.626331329345703, acc 0.43945071472645\n",
      "[305]Test loss 44369.18359375, acc 0.45489050511508955\n",
      "Epoch 306\n",
      "[306]Train loss 15.042999267578125, acc 0.44065513734631384\n",
      "[306]Test loss 45395.83984375, acc 0.44620684143222505\n",
      "Epoch 307\n",
      "[307]Train loss 14.195816040039062, acc 0.4185438234151469\n",
      "[307]Test loss 44681.82421875, acc 0.46802269820971865\n",
      "Epoch 308\n",
      "[308]Train loss 16.633419036865234, acc 0.48641141334156035\n",
      "[308]Test loss 44130.12890625, acc 0.4720947890025575\n",
      "Epoch 309\n",
      "[309]Train loss 16.393991470336914, acc 0.4976589994972349\n",
      "[309]Test loss 45917.21875, acc 0.5090848785166241\n",
      "Epoch 310\n",
      "[310]Train loss 18.40373992919922, acc 0.4553685183280771\n",
      "[310]Test loss 43936.01171875, acc 0.4935477941176471\n",
      "Epoch 311\n",
      "[311]Train loss 21.701068878173828, acc 0.5071283393893687\n",
      "[311]Test loss 45132.21484375, acc 0.49887707800511505\n",
      "Epoch 312\n",
      "[312]Train loss 61.53533935546875, acc 0.4742165004113533\n",
      "[312]Test loss 44693.8515625, acc 0.4796867007672635\n",
      "Epoch 313\n",
      "[313]Train loss 105.29389953613281, acc 0.49224034747931816\n",
      "[313]Test loss 44312.32421875, acc 0.5201806265984655\n",
      "Epoch 314\n",
      "[314]Train loss 61.97418975830078, acc 0.4944342320261438\n",
      "[314]Test loss 44772.19921875, acc 0.5135501918158567\n",
      "Epoch 315\n",
      "[315]Train loss 71.14517211914062, acc 0.487332673453997\n",
      "[315]Test loss 44946.8046875, acc 0.46668718030690537\n",
      "Epoch 316\n",
      "[316]Train loss 57.07087326049805, acc 0.49148727089903554\n",
      "[316]Test loss 44063.11328125, acc 0.49748081841432223\n",
      "Epoch 317\n",
      "[317]Train loss 33.44895553588867, acc 0.4942771178984414\n",
      "[317]Test loss 43975.69921875, acc 0.34537004475703326\n",
      "Epoch 318\n",
      "[318]Train loss 15.231379508972168, acc 0.4344098507701449\n",
      "[318]Test loss 43796.64453125, acc 0.44070652173913044\n",
      "Epoch 319\n",
      "[319]Train loss 12.369444847106934, acc 0.43518363784907904\n",
      "[319]Test loss 43746.73828125, acc 0.5104659526854218\n",
      "Epoch 320\n",
      "[320]Train loss 9.743846893310547, acc 0.5104291643813702\n"
     ]
    }
   ],
   "source": [
    "# make train loop on cpu to reconstruct the model\n",
    "embbeding = MyEmbbeding(df_suic, fake=False)\n",
    "\n",
    "model = NNModel(embbeding, output_dim=len(df_suic.columns))\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    mean_loss = 0\n",
    "    acc = []\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        x, _ = batch\n",
    "        y = x\n",
    "        res = model(x)\n",
    "\n",
    "        # compute loss and backprop using scheduler\n",
    "        loss = criterion(res, y)\n",
    "        loss.backward()\n",
    "        # scheduler.step(loss)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # compute accuracy for each dimension\n",
    "        acc.append([(res[:,i].round() == y[:,i]).sum().item()/len(y) for i in range(y.shape[1])] )\n",
    "\n",
    "        mean_loss += loss\n",
    "    mean_loss /= len(train_dataloader)\n",
    "    acc = np.mean(acc, axis=0)\n",
    "\n",
    "    print(f\"[{epoch}]Train loss {mean_loss}, acc {acc.mean()}\")\n",
    "\n",
    "    # stop at overfitting\n",
    "    if mean_loss < 10:\n",
    "        break\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mean_loss = 0\n",
    "        acc = []\n",
    "        for i, batch in enumerate(test_dataloader):\n",
    "            x, _ = batch\n",
    "            y = x\n",
    "\n",
    "            res = model(x)\n",
    "\n",
    "            # compute loss and backprop using scheduler\n",
    "            loss = criterion(res, y)\n",
    "\n",
    "            # compute accuracy for each dimension\n",
    "            acc.append([(res[:,i].round() == y[:,i]).sum().item()/len(y) for i in range(y.shape[1])] )\n",
    "\n",
    "            mean_loss += loss\n",
    "        mean_loss /= len(test_dataloader)\n",
    "        acc = np.mean(acc, axis=0)\n",
    "\n",
    "        print(f\"[{epoch}]Test loss {mean_loss}, acc {acc.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "# save embbeding\n",
    "torch.save(embbeding.state_dict(), \"embbeding.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use trained embbeding to transform train and test data\n",
    "\n",
    "new_data = []\n",
    "for batch in train_dataloader:\n",
    "    x, y = batch\n",
    "    transformed = embbeding(x)\n",
    "\n",
    "    new_data.append(transformed.detach().numpy())\n",
    "\n",
    "new_data = np.concatenate(new_data, axis=0)\n",
    "\n",
    "# put new data in dataframe with same columns as original data\n",
    "new_df = pd.DataFrame(new_data, columns=df_suic.columns)\n",
    "\n",
    "# save new data\n",
    "new_df.to_csv(\"new_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import causalnex.structure.notears as notears\n",
    "# import networkx as nx\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm = notears.from_pandas(df_suic, max_iter=1000)\n",
    "\n",
    "\n",
    "# ths = [0.1, 0.15, 0.2, 0.3, 0.4, 0.5]\n",
    "# fig, axs = plt.subplots(2, 3, figsize=(30, 20))\n",
    "# for i, ax in enumerate(axs.flatten()):\n",
    "#     if i > len(ths) - 1:\n",
    "#         # Clean up empty axes\n",
    "#         fig.delaxes(ax)\n",
    "#         continue\n",
    "\n",
    "#     th = ths[i]\n",
    "#     sm.remove_edges_below_threshold(th)\n",
    "\n",
    "#     # Draw only nodes with edges\n",
    "#     sm2 = sm.edge_subgraph(sm.edges)\n",
    "#     labels = {node: node for node in sm2.nodes}\n",
    "#     nx.draw(sm, ax=ax, with_labels=True, nodelist=sm2.nodes, labels=labels, node_size=100, font_size=10)\n",
    "\n",
    "#     # Add rec around ax to make it easier to see\n",
    "#     axis = ax.axis()\n",
    "#     rec = plt.Rectangle((axis[0], axis[2]), (axis[1] - axis[0]), (axis[3] - axis[2]), fill=False, lw=4, linestyle=\"dotted\")\n",
    "#     ax.add_patch(rec)\n",
    "#     ax.set_title(f\"Threshold: {th}\")\n",
    "\n",
    "\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "71f78216db5cb2f09350fda35ad9d9668a66a927586f044b9adeddeb17c1df4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
